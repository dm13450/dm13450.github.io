<p>As you move into more complicated Bayesian problems things get more computationally inclined. For most cases, the posterior you arrive at cannot be calculated analytically. Therefore, once you have an expression for your posterior distribution, you need to sample from it to gain an understanding of how it looks.</p>

<p>In this post we will look at the most simple of cases: using Bayesian methods to estimate the mean and variance of data that has been obtained from a normal distribution.</p>

<h2 id="the-likelihood">The Likelihood</h2>

<p>In this simple example we know that the data is normally distributed <script type="math/tex">x_i \sim \mathcal{N} ( \mu , \sigma ^2)</script>. We can define the unknown parameters as a vector <script type="math/tex">\boldsymbol{\theta} = (\mu, \sigma ^2)</script>. The likelihood is obtained by multiplying the distribution together for each data point</p>

<script type="math/tex; mode=display">\mathcal{L} = \prod _i \frac{1}{\sqrt{2 \pi \sigma ^2}} \exp \left( - \frac{(x_i - \mu)^2}{2 \sigma^2} \right),</script>

<script type="math/tex; mode=display">\propto \frac{1}{\sigma ^n} \exp \left(- \frac{1}{2\sigma^2} \sum _i (x_i - \mu )^2 \right).</script>

<p>This can be simplified using the sample variance and sample mean, but this unnessacry detail in this simple case.</p>

<h2 id="the-prior">The Prior</h2>

<p>As there are two unknown parameters, our prior distribution is a joint distribution</p>

<script type="math/tex; mode=display">p(\mu, \sigma ^2) = p(\mu) p (\sigma )</script>

<p>(To be completed)</p>

<h2 id="the-posterior-and-metropolis-sampling">The Posterior and Metropolis Sampling</h2>

<p>Now we multiply the likelihood and the prior together to arrive at the psoterior</p>

<script type="math/tex; mode=display">p(\mu , \sigma ^2 | \underline{x} ) \propto \frac{1}{\sigma _{n+1}} \exp \left( - \frac{1}{2 \sigma ^2} \sum _i (x_i - \mu )^2 \right).</script>

<p>Now to make this a proper probability distribution it would need a normalaisation constant, in this simple case, the constant is trivial. Butwe will not be calculating to illustrate the case when you the constant is not trivial.</p>

<p>Now, how do we understand what the posterior looks like if we haven’t got an exact expression for it? We sample from it, in such a way that the normalising constant isn’t needed. There are a number of alogrithms that can do this, but first we weill start off with Metropolis sampling.</p>

<p>This algorithm works by taking as follows:</p>

<ol>
  <li>Initialise <script type="math/tex">\theta</script></li>
  <li>Propose candidate parameter, <script type="math/tex">\theta _{\text{cand}}</script></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Calculate the ratio of posteriors $$r = \frac{p(\theta _{\text{cand}}</td>
          <td>\bfseries{x})}{p(\theta_{old}</td>
          <td>\bfseries{x})}$$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Calculate $$\alpha = \min \left[ 1, r \right].</li>
  <li>We accept $$\theta _{\text{cand}}$ with probability $\alpha$ and add it to the list of sampled points.</li>
</ol>

<p>The list of <script type="math/tex">\theta</script> is the sample from the posterior.</p>

<h2 id="example">Example</h2>

<p>To see this algorithm in practise it is easy enough to write the functions in R and generate some test data. For this we generate 100 normally distributed points with</p>

