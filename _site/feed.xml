<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dean Markwick</title>
    <description>All rights reserved.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 09 Jan 2017 20:48:14 +0000</pubDate>
    <lastBuildDate>Mon, 09 Jan 2017 20:48:14 +0000</lastBuildDate>
    <generator>Jekyll v3.1.0</generator>
    
      <item>
        <title>Mike Dean and the Ref Radar</title>
        <description>&lt;p&gt;It was a tough day at the office last week for Mike Dean. The match was Manchester United vs West Ham and after 15 minutes he dismissed Feghouli for a foul tackle. The tempo of the game changed. West Ham were on the back foot and eventually conceded two goals to Man Utd. Undeniably the red card had a big effect on the outcome of the game. It has since been rescinded, showing that the FA agree with fans that perhaps it should not have been a red card.&lt;/p&gt;

&lt;p&gt;There has also been a tweet circulating that Mike Dean in his last 7 Tottenham games has sent of three opposition players. On first reading this raises some eyebrows. Is this just a statistical oddity or is there something deeper in the numbers? A prime opportunity to explore our RefRadar of Mike Dean and see how he compares to the other Premier League referees.&lt;/p&gt;

&lt;p&gt;Lets look at the season so far (2016/17) in isolation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadar.png&quot; alt=&quot;Mike Dean Ref Radar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So as we can see here, Mike Dean does like penalising the away team more than the home team for both fouls and yellow cards. Obviously these two stats are going to be correlated. But it is still interesting to see how he is fairly close to the average amount for penalising the home team, but for the away team he is frequently giving fouls and yellow cards out.&lt;/p&gt;

&lt;p&gt;Mike Dean has officiated the 5th most amount of games in the league. So lets look at the 3 other referees who have had more games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadarComp.png&quot; alt=&quot;Mike Dean Ref Radar Comp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most notably, three out of four of the most active referees this season are the ones giving the most yellow cards to the away teams whereas they are middle of the pack for the home yellow cards. A similar trend is seen for the away fouls, but like previous, these two variables are going to be highly correlated.
The stand out statistic though is Mike Deans average away red cards. He is a clearly the most likely to send off a player of the away team out of this selection of referees. In fact the three other referees haven’t sent anyone off from the away team yet this season! But something to keep in mind is that this is a really small sample size. The season is about half way through so there is still plenty of time for the other referees to catch up on the red cards.&lt;/p&gt;

&lt;p&gt;So in conclusion, whatever match Mike Dean is appointed to next, the away team should keep in my mind a red card is never far away. But perhaps the FA are going to have a word with Mike Dean and he will have an easy couple of games to bring his average down. The RefRadar here has been useful to look at how active the officials have been compared to their peers. Lets see what the next round of football brings!&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
        <link>/2017/01/07/Mike-Dean-Ref-Radar.html</link>
        <guid isPermaLink="true">/2017/01/07/Mike-Dean-Ref-Radar.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing the Referee Radar</title>
        <description>&lt;p&gt;The radar plot is a good way to analyse different metrics across a group. The football player radar made popular by &lt;a href=&quot;http://statsbomb.com/2016/04/understand-football-radars-for-mugs-and-muggles/&quot;&gt;Ted Knutson&lt;/a&gt; is good at comparing players and seeing how their stats stack up against one another. Here I will be taking a similar concept and using the radar plot to analyse the referee’s in the professional game in England.&lt;/p&gt;

&lt;p&gt;First, we need to chose some metrics. Using the data from &lt;a href=&quot;http://www.football-data.co.uk/&quot;&gt;Football-Data&lt;/a&gt; we can download all the league matches from the last three seasons. In this data we are privy to the number of fouls, yellow cards and red cards for both the home and away team. That gives us 6 variables for each match and each referee, the perfect amount for a radar plot.&lt;/p&gt;

&lt;p&gt;For each referee we can calculate the average amount of fouls, yellow and red cards the gave to both the home and away team. This will allow us to detect whether a referee is particularly card happy or even has a home or away bias. In terms of practicality, we have to set a threshold for minimum number of games officiated. We remove any ref that has refereed less than 20 games over the three seasons.&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://github.com/ricardo-bion/ggradar&quot;&gt;ggradar&lt;/a&gt; package  and tweaking some of the graphical parameters we are able to come up with the following plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/RefereeRadar.png&quot; alt=&quot;RefereeRadar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we are comparing four referees (at random) and how their metrics personally match against the population of all referees. Here we can see Andy D’Urso is particular for sending an away player off. Simon Hooper is quite a stickler for a foul. Nigel Miller is laid back, not giving many fouls and not giving out the yellow cards either. Chris Kavanagh is very middle of the pack, consistent across home and away for both fouls and cautions.&lt;/p&gt;

&lt;p&gt;The package &lt;code class=&quot;highlighter-rouge&quot;&gt;ggradar&lt;/code&gt; requires all the variables to have a consistent range. For this I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;rescale&lt;/code&gt; function of R to remap the averages to &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;. Therefore the actual values of the metrics are lost in the radar plot. This is something I’ll work on to include in future versions. I’ll also be creating an app, either in shiny or JavaScript that will allow users to compare different referees as they see fit.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/24/Ref-Radar-Intro.html</link>
        <guid isPermaLink="true">/2016/12/24/Ref-Radar-Intro.html</guid>
        
        
      </item>
    
      <item>
        <title>The Data Dialogue - At War with Data</title>
        <description>&lt;p&gt;On Wednesday I took a stroll down to King’s College across the river for a seminar on data and warfare. A bit of a broad subject for a conference. The seminar speakers came from a variety of fields where the innovative usage of data is less obvious and more unique.&lt;/p&gt;

&lt;p&gt;The first talk was by &lt;a href=&quot;http://www.kcl.ac.uk/hr/diversity/meettheprofessors/artshums/croueche.aspx&quot;&gt;Charlotte Rouche&lt;/a&gt; of the Kings college archaeological department. She highlighted how maps, both ancient and current, play an important role in her archaeological research. Understanding how the land is divided and even the names of places in various languages gives an important insight into how civilisation has evolved throughout the ages. Mapping data is now ubiquitous with the likes of Google Maps and Google Earth and this can be useful in observing how areas have developed over time. The aerial photographs of sites of archaeological interest can be periodically monitored for change and help with preservation - fighting the war against decay. However, when cataloguing such data Prof. Rouche raised the importance question of accessibility . Should the public be allowed to see aerial photographs and accurate co-ordinates of culturally and theologically significant monuments? In times of war, this could potentially be a shopping list for attacks. This shows how perhaps a fully open data source is not always the correct answer.&lt;/p&gt;

&lt;p&gt;The second talk was by &lt;a href=&quot;https://www.ucl.ac.uk/spacetimelab/people/kate-bowers&quot;&gt;Kate Bowers&lt;/a&gt; of the Crime Science department at UCL. Her talk was on the usage of police data, which has similar disclosure properties to the Rouche’s map data. Prof. Bowers showed how the GPS data collected from police cars can provide maps of frequent patrol paths of the Metropolitan police. Combining this with 999 call monitoring she was able to give an indication of where there was deficiency in policing were arising. This can lead to policy implementation and a more efficient patrol path for police officers. In this case the war on crime was being fought with this new data.&lt;/p&gt;

&lt;p&gt;The third talk was by &lt;a href=&quot;https://kclpure.kcl.ac.uk/portal/robert.stewart.html&quot;&gt;Robert Stewart&lt;/a&gt;. He is a mental health doctor from the NHS and Professor at King’s. Interestingly, his trust has developed a searchable, anonymous database for mental health researchers. This allowed them to query various parameters across patient groups. A very interesting research tool that required individual patient consent but once granted provides researchers with large amounts of real world clinical data. Its success shows that it is possible to use the health data of patients anonymously without infringing on privacy issues. Currently it is being used to help study drug safety and other questions that required long term monitoring.  Now I have previously worked for a NHS trust in the IT department and have an understanding of different software systems available for the NHS and the various data sharing regulations in place. Therefore this talk was a good insight into how other trusts can conquer some of the data sharing rules and how researchers can get access to such data. As Prof. Stewart is a mental health doctor his work can be framed as the war against mental afflictions.&lt;/p&gt;

&lt;p&gt;The final talk was based on finding adversaries on the internet and looking at how such actors can make themselves known unwillingly. This talk was given by &lt;a href=&quot;http://www.kcl.ac.uk/sspp/departments/warstudies/people/professors/rid.aspx&quot;&gt;Thomas Rid&lt;/a&gt; from the War Studies department at King’s. He offered the example of the Poseta email &lt;a href=&quot;https://en.wikipedia.org/wiki/Podesta_emails&quot;&gt;leak&lt;/a&gt;. By using the classic “reset you password here” phishing scam, Podesta clicked on a bitly link that populated a fake web page with his gmail information and required him to submit his password to change it. Instead, the changing the password button just forwarded his password to the adversary. We know that this was the vector chosen by the attackers due to the public nature of the bitly account. Whoever set the account up forgot to set the profile to private. This led to the account being traced back  to some actor. Who it actually was is up to speculation at the time of writing.&lt;/p&gt;

&lt;p&gt;Overall, it was an enjoyable conference. I learnt about some new domains and in a more focused and “apply-side” way. It was less about algorithm run times and more about what the algorithms actually do and how they can be used.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/07/DataDialogue.html</link>
        <guid isPermaLink="true">/2016/12/07/DataDialogue.html</guid>
        
        
      </item>
    
      <item>
        <title>Notes from a Quantcast Talk</title>
        <description>&lt;p&gt;As I sit on the train to London waiting for my model to finish fitting I realised that it had been a while since I had written a blog post.&lt;/p&gt;

&lt;p&gt;My PhD school holds a bimonthly seminar and this weeks guest speaker was Dr. Peter Day who is a director of engineering at &lt;a href=&quot;https://www.quantcast.com&quot;&gt;Quantcast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Quantcast is a 21st century advertising agency offering real time advertisement strategy and analytic tools. The lecture was based around their use of data and how statistics and infrastructure has helped make advertising more relevant and serve better adds.&lt;/p&gt;

&lt;p&gt;My first &lt;code class=&quot;highlighter-rouge&quot;&gt;did you know&lt;/code&gt; was that the adverts you see on websites are sold in real time as you load the page. So in the time it takes from clicking a link on Goole to the page appearing in your browser, the add space has been bought microseconds earlier and now serving a specific add designed for you. A remarkable engineering achievement, that in the short time it takes from clicking a link to seeing a page that an auction takes place, a winner is found and the add is served. But this is not the main product of Quantcast.&lt;/p&gt;

&lt;p&gt;Instead, Quantcast is more about finding out more about population behaviours and how effective that advert will be. They crunch the necessary data from the variety of cookies they collect and come up with a pricing strategy based on who might see the add. Where as one company might try to show the add to as many people as possible (the fire-hose strategy) Quantcast might drill down on certain factors and change their bidding price based on these factors.&lt;/p&gt;

&lt;p&gt;This problem is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;big data&lt;/code&gt; problem. The amount of data you can collect from cookies; visited websites, location, device etc. can lead to many other inferences which gives a large amount of variables for a large amount of people. Computing this information lead to Quantcast developing their own database and data tools as the current market leaders (Hadoop and derivatives) where not performing well enough. Although they did hint that they was looking at moving to AWS to solve some of their infrastructure problems.&lt;/p&gt;

&lt;p&gt;Overall, it was an interesting talk about a field that I hadn’t really paid that much attention too previously and I definitely learnt something new. It also made me thankful of my add blocker and slightly more paranoid. So thank to Quantcast for coming down and delivering an enjoyable seminar!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/02/Quantcast.html</link>
        <guid isPermaLink="true">/2016/12/02/Quantcast.html</guid>
        
        
      </item>
    
      <item>
        <title>Thoughts on Weights in Bayesian Regression</title>
        <description>&lt;p&gt;Weighted regression is common in frequentist approaches to regression. Typically, the weights are known and can be interpreted as the amount each data point is allowed to influence the link between the variables in question. This can be written as
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \sum _i w_i \beta _i x_i&lt;/script&gt; 
where &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; is the ‘weighting’.&lt;/p&gt;

&lt;p&gt;For example if you have some knowledge that a particular data point is less accurate than all the others. You can assign it a weight such that relative to all the other data points it has less of an impact on the final result. Weighted regression can also be a useful tool in building robust regression models - ones that are less susceptible to outliers as such outliers are given small weights.&lt;/p&gt;

&lt;p&gt;However, when it comes to adding in weights in a Bayesian manner for a regression problem the intuition falls apart.&lt;/p&gt;

&lt;p&gt;Weights in their nature imply that further information is known about the data and the model that it came from. This is a violation of Bayesian thinking as we are no longer considering the sample of data fixed but now dependent on some other function that is manifested in the weights. This weighting function in turn can only be found once we have observed all the data.&lt;/p&gt;

&lt;p&gt;Then by specifying a weighting function we are changing our posterior to be closer to some ideal distribution rather than just letting the data speak for itself. Essentially, changing the results to be closer to what we think the answer should be. Obviously this is not a good practise for any data analysis so we are forced to conclude that weights are not intuitive in a Bayesian way. Instead, if we think that there is a factor influencing the model we should include the factor as a parameter and assess its influence.&lt;/p&gt;

&lt;p&gt;Overall, the fact that weight cannot be thought of as Bayesian is cool little thought experiment and goes to show the divergence between frequentist and Bayesian approaches.&lt;/p&gt;

&lt;p&gt;The man himself, Andrew Gelman, discusses the issue of weighted regression and Bayesian thinking &lt;a href=&quot;https://groups.google.com/forum/#!msg/stan-dev/5pJdH72hoM8/GLW_mTeaObAJ&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 27 Oct 2016 00:00:00 +0100</pubDate>
        <link>/2016/10/27/Weighted-Bayes.html</link>
        <guid isPermaLink="true">/2016/10/27/Weighted-Bayes.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part Two</title>
        <description>&lt;p&gt;In my previous post I have outlined the basics of Kelly betting. Now I will be looking at the optimal bet size for placing bets on multiple simultaneous events that are independent of one another. We will be using R to numerically solve the resulting equations and hopefully learn some quirks of function optimisation in R.&lt;/p&gt;

&lt;p&gt;Again this requires maximising the expected value of the log of the bankroll&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[ \log (x)\right] = \sum _i p_i \log (1+ (b_i-1) x_i) + (1-p_i) \log (1- x_i)&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;where each event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; of occurring, decimal odds of &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is the size of the bet.&lt;/p&gt;

&lt;p&gt;Now that we have multiple bets, the total amount staked must be less than 1&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum _i x_i \leq 1&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;however in practise this is usually capped at some lesser value which is then referred to as fractional Kelly betting.&lt;/p&gt;

&lt;p&gt;Now solving this sum of bets is possible analytically but it is not the easiest nor instructive. Instead, lets turn to maximising the expectation numerically. For this, we turn to R and its optim function.&lt;/p&gt;

&lt;p&gt;Firstly, let us define our expectation function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;due to the vectorised nature of R functions both &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; can be lists and there is no need to loop through each value.&lt;/p&gt;

&lt;p&gt;To find the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values for given &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; values that maximise the bank roll we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;optim&lt;/code&gt; function. &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html&quot;&gt;Optim&lt;/a&gt; is R’s numerical optimisation routine that can implement a number of different algorithms for finding the minimum of a given function. Therefore, for it to be any use to us, we need to multiply our function by -1 such that the maximum now becomes the minimum.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p = c(0.7, 0.8)
b = c(1.3, 1.2)
optim(c(0.5, 0.5), function(x) (-1)*expectedBankRoll(x, p, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code will find the two &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values that maximise the expected bank roll and therefore consider the output as the Kelly bet for two simultaneous results.&lt;/p&gt;

&lt;p&gt;But there is a few caveats. Firstly we need to account for the fact that the sum of our bets must be less than 1. Secondly, the bets must also be positive numbers. To account for these restrictions we must modify our expected bank roll function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  if(sum(x) &amp;gt; 1 | any(x&amp;lt;0)){return(-99999)}
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The returning of a large value if any of the restrictions of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; are broken ensures that we get reasonable results from optim. This also has the added benefit of setting &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; to zero for any event that does not have a positive expected value based on the odds offered. Therefore, to arrive at the optimal bet sizes for a collection of events, just pass in the probabilities and the odds.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at bet hedging and how this can effect the stake size and overall profitability of a betting system.&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/29/Kelly-Betting-Part-Two.html</link>
        <guid isPermaLink="true">/2016/09/29/Kelly-Betting-Part-Two.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part One</title>
        <description>&lt;p&gt;In my current experiments I have been using the Kelly criterion to place theoretical bets on certain events. In the process, I found myself wanting to use the Kelly criterion for multiple simultaneous and independent events but come across a number of problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generally, most of the easily accessible Kelly tutorials only cover betting on one event.&lt;/li&gt;
  &lt;li&gt;Simultaneous Kelly bets are either behind a pay-wall or just a calculator is offered which doesn’t derive any of the results and show how they are obtained.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So like any good scientist I’ve decided to give writing my own guide to Kelly betting. This will be the first part and go through the basic mathematics of the Kelly criterion. The second part will contain the simultaneous Kelly bet methodology.&lt;/p&gt;

&lt;h3 id=&quot;why-kelly-bet&quot;&gt;Why Kelly Bet?&lt;/h3&gt;

&lt;p&gt;Imagine you have a model that predicts the outcome of the event with a probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. You wish to place a bet on this outcome occurring and find that the bookmakers offer (decimal) odds &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. Do you bet the whole house, or are you more conservative? What is the optimal bet size? This was answered by Kelly in 1956.&lt;/p&gt;

&lt;p&gt;To derive the result, we wish to maximise the expected log value of the event. The expected value is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E} \left[ \log X \right] = p \log (1+ (b-1) x) + (1-p) \log (1- x),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the amount that is bet. So to find the value of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; that maximises the expected bank roll we need to do some differentiation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial x} \mathbb{E} \left[ \log X \right] = \frac{p(b-1)}{1 + (b-1)x} - \frac{1-p}{1-x}=0,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(b-1)}{1 + (b-1)x} = \frac{1-p}{1-x},&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \frac{pb-1}{b-1}&lt;/script&gt;

&lt;p&gt;Now if we check the Wikipedia article on Kelly betting we find that this is the same result if we convert from decimal odds to fractional odds. Therefore, for whatever probability your model spits out and whatever the odds the bookmaker offers you, you can place a bet that has a positive expected value and thus probably a good idea.&lt;/p&gt;

&lt;p&gt;If the result from the Kelly formula is negative, this means that you wish to take the other side of the bet. With some betting exchanges, this is possible (“laying odds”). But due to the spread between the back and lay odds, you will not be able to immediately lay at the same odds you can back. Therefore you will need to consider the appropriate Kelly bet for laying an odd.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at multiple bets occurring at the same time and how you can correctly split your bankroll whilst remaining in positive expected value territory.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_criterion&quot;&gt;https://en.wikipedia.org/wiki/Kelly_criterion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&quot;&gt;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/02/Kelly-Betting-Part-One.html</link>
        <guid isPermaLink="true">/2016/09/02/Kelly-Betting-Part-One.html</guid>
        
        
      </item>
    
      <item>
        <title>Dissertation Construction Learnings</title>
        <description>&lt;p&gt;With my dissertation handed in it is time to reflect on what I’ve learnt in terms from constructing the 20,000 word file.&lt;/p&gt;

&lt;p&gt;Firstly, the importance of having an established workflow. If I was to calculate how my time was divided in writing my dissertation an unfortunate amount of time would have been dedicated to collating the many graphs that my work produced. I didn’t have dedicated functions outputting graphs ready to be saved into hard copies at later dates. Instead, I had a variety of functions that produced the results, but using different methods, different representations and different formats. Therefore, for my future work, every time I produce new results which leads to new graphs I need to have functions that can easily output the appropriate data when needed.&lt;/p&gt;

&lt;p&gt;Secondly, my interweaving of Tikz and LaTeX lead to a few troubles. Mainly due to the size of the graphs and lack of available memory for Latex when compiling. My previous method of importing the raw .tex files of the graphs will need to be changed such that all the .tex files are compiled into pdf’s before being imported into Latex. This will also lead to quicker compile times for my Latex document. At some points I was seeing compile times of &amp;gt;5 minutes!&lt;/p&gt;

&lt;p&gt;The use of tables and summarising the final results also lead to some troubles. Copying and pasting the results from the R output to the latex document was not the most efficient use and led to frequent updating each time the results were updated. The solution to this is slightly trickier. Perhaps an interwoven use of RMarkdown and Latex could solve this problem. But at the minute it remains unsolved.&lt;/p&gt;

&lt;p&gt;In an attempt to develop a better workflow I’ve taken to use RMarkdown to construct my research in a more verbose way. This allows me to write around code and output the results straight away without having to translate between R and latex. So far it seems to be working nicely but I’ve yet to try and output to latex.&lt;/p&gt;

&lt;p&gt;Overall, the process of writing my dissertation has shown the importance in establishing good techniques in outputting the results you find and not just focusing on getting the results. With these learnings I’ll be in good preparation for my next dissertation-like project.&lt;/p&gt;

&lt;p&gt;RMarkdown: &lt;a href=&quot;http://rmarkdown.rstudio.com/ &quot;&gt;http://rmarkdown.rstudio.com/ &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Aug 2016 00:00:00 +0100</pubDate>
        <link>/2016/08/17/Dissertation-Learnings.html</link>
        <guid isPermaLink="true">/2016/08/17/Dissertation-Learnings.html</guid>
        
        
      </item>
    
      <item>
        <title>Posterior p-values</title>
        <description>&lt;p&gt;I am now at the point in my work where I need to check my models and whether they correctly describe the data. To do this, lets introduce posterior p-values for a Bayesian model.&lt;/p&gt;

&lt;p&gt;Say we have data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; that we fit using a model &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. As we would have used MCMC to fit the model we have chain of parameter values &lt;script type=&quot;math/tex&quot;&gt;\{\theta ^{(0)} ... \theta ^{(n)}\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For each parameter value we can obtain simulated data &lt;script type=&quot;math/tex&quot;&gt;\hat{y} _i = F( \theta ^{(i)})&lt;/script&gt; such that we now have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simulated data sets.&lt;/p&gt;

&lt;p&gt;We now chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and calculate it for each simulated data set. We can now how &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; compares to the &lt;script type=&quot;math/tex&quot;&gt;T_{\text{sim}}&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; is drastically different from the simulated &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;’s then there is a problem with our model, it is not correctly picking up something intrinsic to the real data.&lt;/p&gt;

&lt;p&gt;Like all good introductions, lets add some real data to try and explain the concepts better.&lt;/p&gt;

&lt;p&gt;Our real data will be simulated from the Generalised Pareto distribution (gpd) and we will fit both an exponential model and a gpd model.&lt;/p&gt;

&lt;p&gt;So now we have three data sets, &lt;script type=&quot;math/tex&quot;&gt;y_{\text{real}}, \hat{y}_{\text{gpd}}, \hat{y}_{\text{exp}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/realandsimdata.png&quot; alt=&quot;Real and Simulated Data&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see straight away that the gpd model has nicely replicated the general shape of the real data, where as the exponential model has produced a poor fit.&lt;/p&gt;

&lt;p&gt;Now we chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. For simplicity we shall use the maximum value of the data set, &lt;script type=&quot;math/tex&quot;&gt;T(x) = \max x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we now calculate the maximum value for all our simulated datatsets of both models and see how the maximum of the real data compares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvaldist.png&quot; alt=&quot;Maximum Value Distributions&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solid black line in the maximum value of the real data set and just by inspection we can reasonably assume that the data is best modelled using a gpd model. Even more so as the x-axis is on a log scale!.&lt;/p&gt;

&lt;p&gt;So this test statistic appears to be suitable of discerning if the data comes from a gpd.&lt;/p&gt;

&lt;p&gt;Now by doing some maths you can calculate the usual power and size of the test statistic, but I’ll save that for a another blog post. This also shows how this method can seem analogous to frequentist p-values.&lt;/p&gt;

&lt;p&gt;Now, lets try using the same method but this time the real data is going to come from an exponential distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvalexp.png&quot; alt=&quot;Maximum Value Distributions Exp&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here our test statistic has failed. This is no obvious difference between the two distributions of the maximum value for the models. Therefore we can not conclude anything. A better test statistic is required!&lt;/p&gt;

&lt;p&gt;So overall, we have shown how to utilise basic test statistics and simulated datasets to analyse the suitability of a model.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&quot;&gt;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Big Red Book of Bayesian Data Analytics (Bayesian Data Analytics by Gelman et al.)&lt;/p&gt;

</description>
        <pubDate>Fri, 20 May 2016 00:00:00 +0100</pubDate>
        <link>/2016/05/20/Posterior-pvalues.html</link>
        <guid isPermaLink="true">/2016/05/20/Posterior-pvalues.html</guid>
        
        
      </item>
    
      <item>
        <title>Bulk Downloading from Turnitin using Python.</title>
        <description>&lt;p&gt;As a teaching assistant, occasionally I get assigned to marks a series of papers. This involves tediously searching for the students paper on the Turnitin app inside moodle before clicking a download button. When you’ve got 36 papers to download, this is far to much clicking and mouse movement. So I wrote a Python script to automate it.&lt;/p&gt;

&lt;p&gt;Firstly, I had to consider how the file was pulled from the server. Thankfully, it was a simple POST request with the paper id as one of the parameters. Using the FireFox addon Tamper, I was able to easily view and submit a custom post request. All it required was a session id and paper id.&lt;/p&gt;

&lt;p&gt;Moving onto Python, I used the urllib2 package to open the custom POST url. Then it was a case of writing the response to a pdf file. Extending this to 36 urls is as simple as looping through each line in a file.&lt;/p&gt;

&lt;p&gt;In Python-esque pseudo-code, this looks like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urllib2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urlopen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sessionid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paperid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The simplicity of the urllib2 is what makes this short script so easy to construct and use.&lt;/p&gt;

&lt;p&gt;Future work would be to get the session id automatically rather than manually copying and pasting it in.&lt;/p&gt;

&lt;p&gt;On an unrelated note, looks like I need to fix the code formatting above. I’ll save that for another day.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/28/Python-Url.html</link>
        <guid isPermaLink="true">/2016/01/28/Python-Url.html</guid>
        
        
      </item>
    
  </channel>
</rss>
