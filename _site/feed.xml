<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dean Markwick</title>
    <description>All rights reserved.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 02 Dec 2016 14:12:44 +0000</pubDate>
    <lastBuildDate>Fri, 02 Dec 2016 14:12:44 +0000</lastBuildDate>
    <generator>Jekyll v3.1.0</generator>
    
      <item>
        <title>Notes from a Quantcast Talk</title>
        <description>&lt;p&gt;As I sit on the train to London waiting for my model to finish fitting I realised that it had been a while since I had written a blog post.&lt;/p&gt;

&lt;p&gt;My PhD school holds a bimonthly seminar and this weeks guest speaker was Dr. Peter Day who is a director of engineering at &lt;a href=&quot;https://www.quantcast.com&quot;&gt;Quantcast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Quantcast is a 21st century advertising agency offering real time advertisement strategy and analytic tools. The lecture was based around their use of data and how statistics and infrastructure has helped make advertising more relevant and serve better adds.&lt;/p&gt;

&lt;p&gt;My first &lt;code class=&quot;highlighter-rouge&quot;&gt;did you know&lt;/code&gt; was that the adverts you see on websites are sold in real time as you load the page. So in the time it takes from clicking a link on Goole to the page appearing in your browser, the add space has been bought microseconds earlier and now serving a specific add designed for you. A remarkable engineering achievement, that in the short time it takes from clicking a link to seeing a page that an auction takes place, a winner is found and the add is served. But this is not the main product of Quantcast.&lt;/p&gt;

&lt;p&gt;Instead, Quantcast is more about finding out more about population behaviours and how effective that advert will be. They crunch the necessary data from the variety of cookies they collect and come up with a pricing strategy based on who might see the add. Where as one company might try to show the add to as many people as possible (the fire-hose strategy) Quantcast might drill down on certain factors and change their bidding price based on these factors.&lt;/p&gt;

&lt;p&gt;This problem is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;big data&lt;/code&gt; problem. The amount of data you can collect from cookies; visited websites, location, device etc. can lead to many other inferences which gives a large amount of variables for a large amount of people. Computing this information lead to Quantcast developing their own database and data tools as the current market leaders (Hadoop and derivatives) where not performing well enough. Although they did hint that they was looking at moving to AWS to solve some of their infrastructure problems.&lt;/p&gt;

&lt;p&gt;Overall, it was an interesting talk about a field that I hadn’t really paid that much attention too previously and I definitely learnt something new. It also made me thankful of my add blocker and slightly more paranoid. So thank to Quantcast for coming down and delivering an enjoyable seminar!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/02/Quantcast.html</link>
        <guid isPermaLink="true">/2016/12/02/Quantcast.html</guid>
        
        
      </item>
    
      <item>
        <title>Thoughts on Weights in Bayesian Regression</title>
        <description>&lt;p&gt;Weighted regression is common in frequentist approaches to regression. Typically, the weights are known and can be interpreted as the amount each data point is allowed to influence the link between the variables in question. This can be written as
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \sum _i w_i \beta _i x_i&lt;/script&gt; 
where &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; is the ‘weighting’.&lt;/p&gt;

&lt;p&gt;For example if you have some knowledge that a particular data point is less accurate than all the others. You can assign it a weight such that relative to all the other data points it has less of an impact on the final result. Weighted regression can also be a useful tool in building robust regression models - ones that are less susceptible to outliers as such outliers are given small weights.&lt;/p&gt;

&lt;p&gt;However, when it comes to adding in weights in a Bayesian manner for a regression problem the intuition falls apart.&lt;/p&gt;

&lt;p&gt;Weights in their nature imply that further information is known about the data and the model that it came from. This is a violation of Bayesian thinking as we are no longer considering the sample of data fixed but now dependent on some other function that is manifested in the weights. This weighting function in turn can only be found once we have observed all the data.&lt;/p&gt;

&lt;p&gt;Then by specifying a weighting function we are changing our posterior to be closer to some ideal distribution rather than just letting the data speak for itself. Essentially, changing the results to be closer to what we think the answer should be. Obviously this is not a good practise for any data analysis so we are forced to conclude that weights are not intuitive in a Bayesian way. Instead, if we think that there is a factor influencing the model we should include the factor as a parameter and assess its influence.&lt;/p&gt;

&lt;p&gt;Overall, the fact that weight cannot be thought of as Bayesian is cool little thought experiment and goes to show the divergence between frequentist and Bayesian approaches.&lt;/p&gt;

&lt;p&gt;The man himself, Andrew Gelman, discusses the issue of weighted regression and Bayesian thinking &lt;a href=&quot;https://groups.google.com/forum/#!msg/stan-dev/5pJdH72hoM8/GLW_mTeaObAJ&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 27 Oct 2016 00:00:00 +0100</pubDate>
        <link>/2016/10/27/Weighted-Bayes.html</link>
        <guid isPermaLink="true">/2016/10/27/Weighted-Bayes.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part Two</title>
        <description>&lt;p&gt;In my previous post I have outlined the basics of Kelly betting. Now I will be looking at the optimal bet size for placing bets on multiple simultaneous events that are independent of one another. We will be using R to numerically solve the resulting equations and hopefully learn some quirks of function optimisation in R.&lt;/p&gt;

&lt;p&gt;Again this requires maximising the expected value of the log of the bankroll&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[ \log (x)\right] = \sum _i p_i \log (1+ (b_i-1) x_i) + (1-p_i) \log (1- x_i)&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;where each event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; of occurring, decimal odds of &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is the size of the bet.&lt;/p&gt;

&lt;p&gt;Now that we have multiple bets, the total amount staked must be less than 1&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum _i x_i \leq 1&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;however in practise this is usually capped at some lesser value which is then referred to as fractional Kelly betting.&lt;/p&gt;

&lt;p&gt;Now solving this sum of bets is possible analytically but it is not the easiest nor instructive. Instead, lets turn to maximising the expectation numerically. For this, we turn to R and its optim function.&lt;/p&gt;

&lt;p&gt;Firstly, let us define our expectation function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;due to the vectorised nature of R functions both &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; can be lists and there is no need to loop through each value.&lt;/p&gt;

&lt;p&gt;To find the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values for given &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; values that maximise the bank roll we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;optim&lt;/code&gt; function. &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html&quot;&gt;Optim&lt;/a&gt; is R’s numerical optimisation routine that can implement a number of different algorithms for finding the minimum of a given function. Therefore, for it to be any use to us, we need to multiply our function by -1 such that the maximum now becomes the minimum.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p = c(0.7, 0.8)
b = c(1.3, 1.2)
optim(c(0.5, 0.5), function(x) (-1)*expectedBankRoll(x, p, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code will find the two &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values that maximise the expected bank roll and therefore consider the output as the Kelly bet for two simultaneous results.&lt;/p&gt;

&lt;p&gt;But there is a few caveats. Firstly we need to account for the fact that the sum of our bets must be less than 1. Secondly, the bets must also be positive numbers. To account for these restrictions we must modify our expected bank roll function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  if(sum(x) &amp;gt; 1 | any(x&amp;lt;0)){return(-99999)}
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The returning of a large value if any of the restrictions of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; are broken ensures that we get reasonable results from optim. This also has the added benefit of setting &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; to zero for any event that does not have a positive expected value based on the odds offered. Therefore, to arrive at the optimal bet sizes for a collection of events, just pass in the probabilities and the odds.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at bet hedging and how this can effect the stake size and overall profitability of a betting system.&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/29/Kelly-Betting-Part-Two.html</link>
        <guid isPermaLink="true">/2016/09/29/Kelly-Betting-Part-Two.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part One</title>
        <description>&lt;p&gt;In my current experiments I have been using the Kelly criterion to place theoretical bets on certain events. In the process, I found myself wanting to use the Kelly criterion for multiple simultaneous and independent events but come across a number of problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generally, most of the easily accessible Kelly tutorials only cover betting on one event.&lt;/li&gt;
  &lt;li&gt;Simultaneous Kelly bets are either behind a pay-wall or just a calculator is offered which doesn’t derive any of the results and show how they are obtained.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So like any good scientist I’ve decided to give writing my own guide to Kelly betting. This will be the first part and go through the basic mathematics of the Kelly criterion. The second part will contain the simultaneous Kelly bet methodology.&lt;/p&gt;

&lt;h3 id=&quot;why-kelly-bet&quot;&gt;Why Kelly Bet?&lt;/h3&gt;

&lt;p&gt;Imagine you have a model that predicts the outcome of the event with a probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. You wish to place a bet on this outcome occurring and find that the bookmakers offer (decimal) odds &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. Do you bet the whole house, or are you more conservative? What is the optimal bet size? This was answered by Kelly in 1956.&lt;/p&gt;

&lt;p&gt;To derive the result, we wish to maximise the expected log value of the event. The expected value is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E} \left[ \log X \right] = p \log (1+ (b-1) x) + (1-p) \log (1- x),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the amount that is bet. So to find the value of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; that maximises the expected bank roll we need to do some differentiation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial x} \mathbb{E} \left[ \log X \right] = \frac{p(b-1)}{1 + (b-1)x} - \frac{1-p}{1-x}=0,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(b-1)}{1 + (b-1)x} = \frac{1-p}{1-x},&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \frac{pb-1}{b-1}&lt;/script&gt;

&lt;p&gt;Now if we check the Wikipedia article on Kelly betting we find that this is the same result if we convert from decimal odds to fractional odds. Therefore, for whatever probability your model spits out and whatever the odds the bookmaker offers you, you can place a bet that has a positive expected value and thus probably a good idea.&lt;/p&gt;

&lt;p&gt;If the result from the Kelly formula is negative, this means that you wish to take the other side of the bet. With some betting exchanges, this is possible (“laying odds”). But due to the spread between the back and lay odds, you will not be able to immediately lay at the same odds you can back. Therefore you will need to consider the appropriate Kelly bet for laying an odd.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at multiple bets occurring at the same time and how you can correctly split your bankroll whilst remaining in positive expected value territory.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_criterion&quot;&gt;https://en.wikipedia.org/wiki/Kelly_criterion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&quot;&gt;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/02/Kelly-Betting-Part-One.html</link>
        <guid isPermaLink="true">/2016/09/02/Kelly-Betting-Part-One.html</guid>
        
        
      </item>
    
      <item>
        <title>Dissertation Construction Learnings</title>
        <description>&lt;p&gt;With my dissertation handed in it is time to reflect on what I’ve learnt in terms from constructing the 20,000 word file.&lt;/p&gt;

&lt;p&gt;Firstly, the importance of having an established workflow. If I was to calculate how my time was divided in writing my dissertation an unfortunate amount of time would have been dedicated to collating the many graphs that my work produced. I didn’t have dedicated functions outputting graphs ready to be saved into hard copies at later dates. Instead, I had a variety of functions that produced the results, but using different methods, different representations and different formats. Therefore, for my future work, every time I produce new results which leads to new graphs I need to have functions that can easily output the appropriate data when needed.&lt;/p&gt;

&lt;p&gt;Secondly, my interweaving of Tikz and LaTeX lead to a few troubles. Mainly due to the size of the graphs and lack of available memory for Latex when compiling. My previous method of importing the raw .tex files of the graphs will need to be changed such that all the .tex files are compiled into pdf’s before being imported into Latex. This will also lead to quicker compile times for my Latex document. At some points I was seeing compile times of &amp;gt;5 minutes!&lt;/p&gt;

&lt;p&gt;The use of tables and summarising the final results also lead to some troubles. Copying and pasting the results from the R output to the latex document was not the most efficient use and led to frequent updating each time the results were updated. The solution to this is slightly trickier. Perhaps an interwoven use of RMarkdown and Latex could solve this problem. But at the minute it remains unsolved.&lt;/p&gt;

&lt;p&gt;In an attempt to develop a better workflow I’ve taken to use RMarkdown to construct my research in a more verbose way. This allows me to write around code and output the results straight away without having to translate between R and latex. So far it seems to be working nicely but I’ve yet to try and output to latex.&lt;/p&gt;

&lt;p&gt;Overall, the process of writing my dissertation has shown the importance in establishing good techniques in outputting the results you find and not just focusing on getting the results. With these learnings I’ll be in good preparation for my next dissertation-like project.&lt;/p&gt;

&lt;p&gt;RMarkdown: &lt;a href=&quot;http://rmarkdown.rstudio.com/ &quot;&gt;http://rmarkdown.rstudio.com/ &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Aug 2016 00:00:00 +0100</pubDate>
        <link>/2016/08/17/Dissertation-Learnings.html</link>
        <guid isPermaLink="true">/2016/08/17/Dissertation-Learnings.html</guid>
        
        
      </item>
    
      <item>
        <title>Posterior p-values</title>
        <description>&lt;p&gt;I am now at the point in my work where I need to check my models and whether they correctly describe the data. To do this, lets introduce posterior p-values for a Bayesian model.&lt;/p&gt;

&lt;p&gt;Say we have data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; that we fit using a model &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. As we would have used MCMC to fit the model we have chain of parameter values &lt;script type=&quot;math/tex&quot;&gt;\{\theta ^{(0)} ... \theta ^{(n)}\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For each parameter value we can obtain simulated data &lt;script type=&quot;math/tex&quot;&gt;\hat{y} _i = F( \theta ^{(i)})&lt;/script&gt; such that we now have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simulated data sets.&lt;/p&gt;

&lt;p&gt;We now chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and calculate it for each simulated data set. We can now how &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; compares to the &lt;script type=&quot;math/tex&quot;&gt;T_{\text{sim}}&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; is drastically different from the simulated &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;’s then there is a problem with our model, it is not correctly picking up something intrinsic to the real data.&lt;/p&gt;

&lt;p&gt;Like all good introductions, lets add some real data to try and explain the concepts better.&lt;/p&gt;

&lt;p&gt;Our real data will be simulated from the Generalised Pareto distribution (gpd) and we will fit both an exponential model and a gpd model.&lt;/p&gt;

&lt;p&gt;So now we have three data sets, &lt;script type=&quot;math/tex&quot;&gt;y_{\text{real}}, \hat{y}_{\text{gpd}}, \hat{y}_{\text{exp}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/realandsimdata.png&quot; alt=&quot;Real and Simulated Data&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see straight away that the gpd model has nicely replicated the general shape of the real data, where as the exponential model has produced a poor fit.&lt;/p&gt;

&lt;p&gt;Now we chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. For simplicity we shall use the maximum value of the data set, &lt;script type=&quot;math/tex&quot;&gt;T(x) = \max x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we now calculate the maximum value for all our simulated datatsets of both models and see how the maximum of the real data compares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvaldist.png&quot; alt=&quot;Maximum Value Distributions&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solid black line in the maximum value of the real data set and just by inspection we can reasonably assume that the data is best modelled using a gpd model. Even more so as the x-axis is on a log scale!.&lt;/p&gt;

&lt;p&gt;So this test statistic appears to be suitable of discerning if the data comes from a gpd.&lt;/p&gt;

&lt;p&gt;Now by doing some maths you can calculate the usual power and size of the test statistic, but I’ll save that for a another blog post. This also shows how this method can seem analogous to frequentist p-values.&lt;/p&gt;

&lt;p&gt;Now, lets try using the same method but this time the real data is going to come from an exponential distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvalexp.png&quot; alt=&quot;Maximum Value Distributions Exp&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here our test statistic has failed. This is no obvious difference between the two distributions of the maximum value for the models. Therefore we can not conclude anything. A better test statistic is required!&lt;/p&gt;

&lt;p&gt;So overall, we have shown how to utilise basic test statistics and simulated datasets to analyse the suitability of a model.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&quot;&gt;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Big Red Book of Bayesian Data Analytics (Bayesian Data Analytics by Gelman et al.)&lt;/p&gt;

</description>
        <pubDate>Fri, 20 May 2016 00:00:00 +0100</pubDate>
        <link>/2016/05/20/Posterior-pvalues.html</link>
        <guid isPermaLink="true">/2016/05/20/Posterior-pvalues.html</guid>
        
        
      </item>
    
      <item>
        <title>Bulk Downloading from Turnitin using Python.</title>
        <description>&lt;p&gt;As a teaching assistant, occasionally I get assigned to marks a series of papers. This involves tediously searching for the students paper on the Turnitin app inside moodle before clicking a download button. When you’ve got 36 papers to download, this is far to much clicking and mouse movement. So I wrote a Python script to automate it.&lt;/p&gt;

&lt;p&gt;Firstly, I had to consider how the file was pulled from the server. Thankfully, it was a simple POST request with the paper id as one of the parameters. Using the FireFox addon Tamper, I was able to easily view and submit a custom post request. All it required was a session id and paper id.&lt;/p&gt;

&lt;p&gt;Moving onto Python, I used the urllib2 package to open the custom POST url. Then it was a case of writing the response to a pdf file. Extending this to 36 urls is as simple as looping through each line in a file.&lt;/p&gt;

&lt;p&gt;In Python-esque pseudo-code, this looks like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urllib2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urlopen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sessionid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paperid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The simplicity of the urllib2 is what makes this short script so easy to construct and use.&lt;/p&gt;

&lt;p&gt;Future work would be to get the session id automatically rather than manually copying and pasting it in.&lt;/p&gt;

&lt;p&gt;On an unrelated note, looks like I need to fix the code formatting above. I’ll save that for another day.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/28/Python-Url.html</link>
        <guid isPermaLink="true">/2016/01/28/Python-Url.html</guid>
        
        
      </item>
    
      <item>
        <title>Films I&#39;ve Watched In January 2016</title>
        <description>&lt;p&gt;Ordered as per my memory!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;V for Vendetta&lt;/li&gt;
  &lt;li&gt;Cartel Land&lt;/li&gt;
  &lt;li&gt;Howl&lt;/li&gt;
  &lt;li&gt;Silence of The Lambs&lt;/li&gt;
  &lt;li&gt;Sicario&lt;/li&gt;
  &lt;li&gt;Star Wars 7&lt;/li&gt;
  &lt;li&gt;Legend&lt;/li&gt;
  &lt;li&gt;Mad Max: Fury Road&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Silence of the Lambs easily comes out top and I’ll be the millionth person to say that the acting and story is captivating. If you’ve not seen it, put it high on your list of things to watch.&lt;/p&gt;

&lt;p&gt;Mad Max is the most over rated. Despite the various internet list raving about it the non-stop action and lack of real character development just didn’t click with me. The fact that they ended up turning around on the road just to go back again felt like a bit of a cop out.&lt;/p&gt;

&lt;p&gt;Legend was another underwhelming occurrence. I felt like it was trying to appeal to the separate male and female audiences in tandem and not really connecting with either. Whilst the technical effect of having Tom Hardy play both roles was interesting, it wasn’t enough to salvage the story. In the end it was the worst of both a love story and a gangster film.&lt;/p&gt;

&lt;p&gt;Similarly, Sicario started off as a cop/military film before throwing that all out the window to become a revenge film. The first half is intense and lets you develop a real distrust of the authority figures, but this eventually fizzles out as the film moves onto the revenge part before ending with either questions for the audience to think about themselves but they actually feel like plot holes. Still, it peaked my interest in the Mexican Cartel story, hence I gave Cartel Land a watch. It’s an alright documentary, but less about the Cartels and more about the resistance against them.&lt;/p&gt;

&lt;p&gt;My only trip to the cinema in January was to beat the crowds and finally see the behemoth that is the new Star Wars. It does everything right but it doesn’t push any boundaries. The more I think of it, the more I realise that it is basically a HD remake of A New Hope. But still, I look forward to number 8 and see whether that is a bit more adventurous.&lt;/p&gt;

&lt;p&gt;Finally, Howl, your classic lower budget horror film. It ticks the boxes and provides enough core and creature shots to leave you feeling satisfied. Watch it as an appetiser to Dog Soldiers.&lt;/p&gt;

</description>
        <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/25/January-Films.html</link>
        <guid isPermaLink="true">/2016/01/25/January-Films.html</guid>
        
        
      </item>
    
      <item>
        <title>The First Workshop</title>
        <description>&lt;p&gt;On Friday the 25th of September I took myself along to King’s College to attend the London-Paris Bachelier Workshop on Mathematical Finance. It was a two day event designed to highlight the collaboration between London and Parisian universities in the maths and finance field. As a new graduate student of this field I thought it was a good opportunity to attend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/londonparis.jpg&quot; alt=&quot;My badge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whilst there were many interesting topics, the stand out talk for me was John Armstrong on stochastic filtering. By projecting a stochastic equation onto a manifold and choosing an appropriate metric, an approximate solution can be found. The exact details are a bit beyond my grasp and I hope that I haven’t misinterpreted the overall aim of the talk. I hope that in the future I can look back on this and re-write it with a better understanding!&lt;/p&gt;

&lt;p&gt;Overall, the workshop was an excellent introduction to what to expect as a grad student and I look forward to attending more.&lt;/p&gt;

</description>
        <pubDate>Wed, 07 Oct 2015 00:00:00 +0100</pubDate>
        <link>/2015/10/07/The-First-Workshop.html</link>
        <guid isPermaLink="true">/2015/10/07/The-First-Workshop.html</guid>
        
        
      </item>
    
      <item>
        <title>Prop Shop Interviews</title>
        <description>&lt;p&gt;I thought I would try my hand at applying to a few proprietary trading firms in London. Having read Scott Pattersons books (Dark Pools and Quants) I felt like I had a decent grasp on the industry and felt that my skills from physics would set me up well for a trading job.&lt;/p&gt;

&lt;p&gt;What I didn’t bank on was a minefield of an interview experience that involved calculator like tasks, running through matrix algebra and computer science terms that I had a ‘brief skim over wikipedia’ knowledge of. So here’s a run down of both interviews for each company.&lt;/p&gt;

&lt;p&gt;The first one was an hour long, asked a few questions of my experience at UBS before jumping into the maths.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Consider a bag with 3 balls; each a different colour. You remove 2 balls, paint them one colour and replace the balls in the bag. What’s the expected amount of moves before all the balls are the same colour?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My first instinct was to set up the transition matrix and take it from there. Points for correctly identifying that it was a Markov chain.&lt;/p&gt;

&lt;p&gt;I believed that the transition matrix would be a 3x3 matrix as there are three possible states that the bag could be in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the balls are different colours.&lt;/li&gt;
  &lt;li&gt;One ball is a different colour to the other two.&lt;/li&gt;
  &lt;li&gt;All the balls are the same colour.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is correct, but not the ‘smart’ way to do. As the first move always takes the system from state 1 to state 2 you can simplify the matrix to just a 2x2 but remembering to add one to the average at the end.&lt;/p&gt;

&lt;p&gt;Now to calculate the transition probabilities. If we call state 1 the state in which there are two of one colour and one of the other and state 2 as all balls the same colour.&lt;/p&gt;

&lt;p&gt;From state 1, there are two possibilities when drawing two balls. You pick two balls that are the same colour; this returns you too the same state. You chose two different colour balls, if you paint them the same colour as the remaining one, then the system is in the final state. If you paint them one of the other two colours, then the system stays in the same state.&lt;/p&gt;

&lt;p&gt;This amounts to the probability of the system becoming fixed after one move:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\text{fixed}) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{2}{9}&lt;/script&gt;

&lt;p&gt;Therefore the expected number of transitions is the reciprocal of this fixation probability. BUT plus an extra one, due to the first move.&lt;/p&gt;

&lt;p&gt;Therefore the average number of transitions is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N = \frac{9}{2} + 1 = \frac{11}{2}&lt;/script&gt;

&lt;p&gt;The second question was to write a function that would reverse a linked list. This was a bit daunting seens as my familiarity with linked lists is next to none. But, in truth, I understand what pointers are, therefore a linked list isn’t something too abstract.&lt;/p&gt;

&lt;p&gt;My first thought out loud ended up transpiring to reading the node pointers one by one into a new array and then returning the array. This is wrong, as the function then doesn’t return a linked list explicitly, instead just a list of pointers. It then clicked that I was going to have to change what way the pointer was pointing for each element in the list and this would involve a temporary variable. However, my use of temporary variable on reflection was not correct and whilst in the process of writing this post I realise that my solution was very incorrect.&lt;/p&gt;

&lt;p&gt;For the second interview, the questions asked were much less involved and basic maths but where you had to invoke a few tricks. For example, &lt;script type=&quot;math/tex&quot;&gt;41 \times 39&lt;/script&gt; is daunting withouta calculator, or at least it was to me. However, it can be simplified to &lt;script type=&quot;math/tex&quot;&gt;(40+1)(40-1) = 40 ^2 - 1 = 1599&lt;/script&gt;. After four questions were answered, I was then asked to estimate my confidence that I had answered them all correctly. I was 100% confident on 3 of them but less so on 1 other, therefore I believe a 75% confidence level would be sufficient.&lt;/p&gt;

&lt;p&gt;Overall, both interviews have highlighted the skills needed to succeed in these types of interviews. Tautology, but both times I asked catheter the interviewer used any of the topics in their daily work. They said no.&lt;/p&gt;

</description>
        <pubDate>Sun, 30 Nov 2014 00:00:00 +0000</pubDate>
        <link>/2014/11/30/Trading-Interviews.html</link>
        <guid isPermaLink="true">/2014/11/30/Trading-Interviews.html</guid>
        
        
      </item>
    
  </channel>
</rss>
