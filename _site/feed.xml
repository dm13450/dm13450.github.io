<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dean Markwick</title>
    <description>All rights reserved.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 29 Sep 2016 20:52:54 +0100</pubDate>
    <lastBuildDate>Thu, 29 Sep 2016 20:52:54 +0100</lastBuildDate>
    <generator>Jekyll v3.1.0</generator>
    
      <item>
        <title>Inhomogenous Poisson Process</title>
        <description>&lt;p&gt;My first forray into statistics and finance begins with the simulation, fitting and checking of the inhomogenous poisson process.
THe inhomogenous poisson process is easily defined, instead of constant rate $\lambda$ as per the usual Poisson process, the rate can now varry in time.&lt;/p&gt;

&lt;p&gt;Simulating this is done using a method called thinning; a Poisson process is simulated with rate greater than the inhomogenous rate, with points being rejected, or “thinned” out to give the final inhomogenous process.&lt;/p&gt;

&lt;p&gt;Therefore the algorithm runs as follows: 
1. Generate a Poisson varaible with rate $\lambda ^&lt;em&gt;$ such that $\lambda ^&lt;/em&gt; &amp;gt; \lambda (t) \forall t$. Increment time by this variable. 
2. Calculate the ratio $\lambda (t) / \lambda ^*$ and compare this to a unifromally distributed variable. If the ratio is greater then accept the time else reject the time and continue with step one again. 
3. The accepted times are the correct times of the inhomogenous process.&lt;/p&gt;

</description>
        <pubDate>Thu, 29 Sep 2016 20:52:54 +0100</pubDate>
        <link>/2016/09/29/InhomoPoisson.html</link>
        <guid isPermaLink="true">/2016/09/29/InhomoPoisson.html</guid>
        
        
      </item>
    
      <item>
        <title>Febfilms</title>
        <description>&lt;hr /&gt;
&lt;p&gt;layout: post
title: Films and Books of February
date: 2016-02-03
***&lt;/p&gt;

&lt;p&gt;Carrying from last months post and an in an attempt to make a habit out of writing I am going to have a look at the films and books consumed in February. 
This time I’ve ordered them roughly on how good they are.&lt;/p&gt;

&lt;p&gt;** Films&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spotlight&lt;/li&gt;
  &lt;li&gt;The Big Short&lt;/li&gt;
  &lt;li&gt;What We Do In The Shadows&lt;/li&gt;
  &lt;li&gt;Edge of Tomorrow&lt;/li&gt;
  &lt;li&gt;Alien and Aliens&lt;/li&gt;
  &lt;li&gt;Bridge of Spies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spotlight and The Big Short are going to find themselves tough to beat in 2016. Both stellar films highlighting the state of corruption in our society albeit with different tones.&lt;/p&gt;

&lt;p&gt;Moving onto the more lighthearted films, What We Do In The Shadows (WWDITS) is an original, low budgety comedy that will make you laugh. It takes on the mockumentary approach adds a supernatural spin. It takes all the good parts from The Inbetweeners, The Office and vampire lore and package them into a film. This is what WWDITS essentially is. I look forward to a sequal and more from the director. 
Speaking again of originallity, Edge of Tomorrow will take any viewer back to there childhood. It’s got robots, aliens, high tech weaponary and a respawning mechanic. If you want a film to watch on a saturday night after the X-Factor has finished you’d be hard pressed to find something as enjoyable as Edge of Tomorrow.&lt;/p&gt;

&lt;p&gt;Bridge of Spies fell a bit to me. Like last months Legend viewing, Bridge of Spies couldn’t deciede whether it wanted to be a courtroom drama or more focused on the negotiation aspect between East Germany and Russia. As a result you got two unsatisfactory parts of the film. It also had the usual Spielberg quirks: a moment of two people talking over each other, the main character going from being hated to being loved as if public opinion is that black and white. By all means it is a good film, but nothing more. Watch it once and continue with your life.&lt;/p&gt;

&lt;p&gt;Finally, inspired by a play-through of Alien: Isolation on the PS4 I decieded to eduacte myself in the actual films. Alien had the very ‘classical’ sci-fi feel to it, with the beeping terminals, robots so realistic they pass of as humans and finally actual people in rubber suits opposed to CGI. Watching it takes you into that world and makes you nostalgic for the alternate timeline where computers fill rooms and sleeping pods. The influence it has on cinema is easily digested and thankfully doesn’t move into the ‘Seinfield is unfunny’ trope. Similarly, Aliens takes the same angle, although much less delicate. There are even paralels to Edge of Tomorrow and Aliens.&lt;/p&gt;

&lt;p&gt;** Books&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Boys&lt;/li&gt;
  &lt;li&gt;Chris Hadfield’s Autobiography&lt;/li&gt;
  &lt;li&gt;The Art of Deception&lt;/li&gt;
  &lt;li&gt;Crytopnomicon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout January I was a slave to the behomoth that is Cryprtonomicon and I’m ashamed to say that I still haven’t finished it. I’m 600 pages in and still feel like the proper story is yet to start. Everynow and then one of the characters goes on an arc that has me turning pages, but that usually finishes at the end of the chapter and then the book loses momentum and I find myself looking at the back trying to work out how many pages I need to read a day to finish before the next leap year. For this reason I think I’m going to have to relegate Crytonomicon to the unfinished pile.&lt;/p&gt;

&lt;p&gt;Moving onto the non-fiction books it has been a mixture of finance, space and security. 
With the debate of IEX continually appearing on my Twitter timeline and my Phd work touching it’s toes in microstructure, Flash Boys has been on my to-read list for a while. I’m still strugling to form an opinion on the morality of HFT even after reading it. I feel like the majority of what is described in Flash Boys can be categorised as market impact. If you place a large order, other traders are going to adjust there prices. The finite nature of passing messages back and forth from exchanges means that there will be information asymetry and thus arbitrage oppurunities.&lt;/p&gt;

&lt;p&gt;From low latency to high latency, Chris Hadfield’s book is a simple and consice account of his time training and actual time in space on various missions. I enjoyed it for the insight into the organisation of NASA and how as a Canadian there is a slightly different path to becoming a spaceman. I imagine it’s similar to the UK and becoming part of the ESA crew. 
Whilst the book is an easy read, I found it a bit light on the details and prehaps a bit of negativity to give it a bit of balance. It felt much like a good PR read for NASA and many of it’s weaknesses were actually it’s strenghts. I.e. the flat organisational structure that means that returning astronaughts are placed bottom of the pile. Much like Alex Fergusan said, no player bigger than the club - no astronaught bigger than NASA.&lt;/p&gt;

</description>
        <pubDate>Thu, 29 Sep 2016 20:52:54 +0100</pubDate>
        <link>/2016/09/29/FebFilms.html</link>
        <guid isPermaLink="true">/2016/09/29/FebFilms.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part Two</title>
        <description>&lt;p&gt;In my previous post I have outlined the basics of Kelly betting. Now I will be looking at the optimal bet size for placing bets on multiple simultaneous events that are independent of one another. We will be using R to numerically solve the resulting equations and hopefully learn some quirks of function optimisation in R.&lt;/p&gt;

&lt;p&gt;Again this requires maximising the expected value of the log of the bankroll&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[ \log (x)\right] = \sum _i p_i \log (1+ (b_i-1) x_i) + (1-p_i) \log (1- x_i)&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;where each event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; of occurring, decimal odds of &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is the size of the bet.&lt;/p&gt;

&lt;p&gt;Now that we have multiple bets, the total amount staked must be less than 1&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum _i x_i \leq 1&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;however in practise this is usually capped at some lesser value which is then referred to as fractional Kelly betting.&lt;/p&gt;

&lt;p&gt;Now solving this sum of bets is possible analytically but it is not the easiest nor instructive. Instead, lets turn to maximising the expectation numerically. For this, we turn to R and its optim function.&lt;/p&gt;

&lt;p&gt;Firstly, let us define our expectation function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;due to the vectorised nature of R functions both &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; can be lists and there is no need to loop through each value.&lt;/p&gt;

&lt;p&gt;To find the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values for given &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; values that maximise the bank roll we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;optim&lt;/code&gt; function. &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html&quot;&gt;Optim&lt;/a&gt; is R’s numerical optimisation routine that can implement a number of different algorithms for finding the minimum of a given function. Therefore, for it to be any use to us, we need to multiply our function by -1 such that the maximum now becomes the minimum.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p = c(0.7, 0.8)
b = c(1.3, 1.2)
optim(c(0.5, 0.5), function(x) (-1)*expectedBankRoll(x, p, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code will find the two &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values that maximise the expected bank roll and therefore consider the output as the Kelly bet for two simultaneous results.&lt;/p&gt;

&lt;p&gt;But there is a few caveats. Firstly we need to account for the fact that the sum of our bets must be less than 1. Secondly, the bets must also be positive numbers. To account for these restrictions we must modify our expected bank roll function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  if(sum(x) &amp;gt; 1 | any(x&amp;lt;0)){return(-99999)}
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The returning of a large value if any of the restrictions of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; are broken ensures that we get reasonable results from optim. This also has the added benefit of setting &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; to zero for any event that does not have a positive expected value based on the odds offered. Therefore, to arrive at the optimal bet sizes for a collection of events, just pass in the probabilities and the odds.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at bet hedging and how this can effect the stake size and overall profitability of a betting system.&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/29/Kelly-Betting-Part-Two.html</link>
        <guid isPermaLink="true">/2016/09/29/Kelly-Betting-Part-Two.html</guid>
        
        
      </item>
    
      <item>
        <title>Thoughts on Weights in Bayesian Regression</title>
        <description>&lt;p&gt;Weighted regression is common in frequentist approaches to regression. Typically, the weights are known and can be interpreted as the amount each datapoint is allowed to influence the link between the variables in question. For example if you have some knowlege that a particular datapoint is less accurate than all the others you might assign it a weight such that relative to all the other data point it has less of an impact on the final result. Weighted regression can also be a useful tool in building robust regression models - ones that are less suseceptable to outliers. However, when it comes to adding in weights in a Bayesian regression problem the intuition falls apart.&lt;/p&gt;

&lt;p&gt;Weights in their nature imply that further information is known about the data and the model that it came from. This is a violation of Bayesian thinking as we are no longer considering the sample of data fixed but now dependent on some other function that is manifested in the weights. This weighting function in turn can only be found once we have observed all the data.&lt;/p&gt;

&lt;p&gt;The man himself, Andrew Gelman, discusses the issue of weighted regression and Bayesian thinking here https://groups.google.com/forum/#!msg/stan-dev/5pJdH72hoM8/GLW_mTeaObAJ&lt;/p&gt;

</description>
        <pubDate>Wed, 28 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/28/WeightedBayes.html</link>
        <guid isPermaLink="true">/2016/09/28/WeightedBayes.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part One</title>
        <description>&lt;p&gt;In my current experiments I have been using the Kelly criterion to place theoretical bets on certain events. In the process, I found myself wanting to use the Kelly criterion for multiple simultaneous and independent events but come across a number of problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generally, most of the easily accessible Kelly tutorials only cover betting on one event.&lt;/li&gt;
  &lt;li&gt;Simultaneous Kelly bets are either behind a pay-wall or just a calculator is offered which doesn’t derive any of the results and show how they are obtained.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So like any good scientist I’ve decided to give writing my own guide to Kelly betting. This will be the first part and go through the basic mathematics of the Kelly criterion. The second part will contain the simultaneous Kelly bet methodology.&lt;/p&gt;

&lt;h3 id=&quot;why-kelly-bet&quot;&gt;Why Kelly Bet?&lt;/h3&gt;

&lt;p&gt;Imagine you have a model that predicts the outcome of the event with a probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. You wish to place a bet on this outcome occurring and find that the bookmakers offer (decimal) odds &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. Do you bet the whole house, or are you more conservative? What is the optimal bet size? This was answered by Kelly in 1956.&lt;/p&gt;

&lt;p&gt;To derive the result, we wish to maximise the expected log value of the event. The expected value is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E} \left[ \log X \right] = p \log (1+ (b-1) x) + (1-p) \log (1- x),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the amount that is bet. So to find the value of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; that maximises the expected bank roll we need to do some differentiation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial x} \mathbb{E} \left[ \log X \right] = \frac{p(b-1)}{1 + (b-1)x} - \frac{1-p}{1-x}=0,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(b-1)}{1 + (b-1)x} = \frac{1-p}{1-x},&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \frac{pb-1}{b-1}&lt;/script&gt;

&lt;p&gt;Now if we check the Wikipedia article on Kelly betting we find that this is the same result if we convert from decimal odds to fractional odds. Therefore, for whatever probability your model spits out and whatever the odds the bookmaker offers you, you can place a bet that has a positive expected value and thus probably a good idea.&lt;/p&gt;

&lt;p&gt;If the result from the Kelly formula is negative, this means that you wish to take the other side of the bet. With some betting exchanges, this is possible (“laying odds”). But due to the spread between the back and lay odds, you will not be able to immediately lay at the same odds you can back. Therefore you will need to consider the appropriate Kelly bet for laying an odd.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at multiple bets occurring at the same time and how you can correctly split your bankroll whilst remaining in positive expected value territory.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kelly_criterion&quot;&gt;https://en.wikipedia.org/wiki/Kelly_criterion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&quot;&gt;http://www.sportsbookreview.com/picks/tools/kelly-calculator/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/02/Kelly-Betting-Part-One.html</link>
        <guid isPermaLink="true">/2016/09/02/Kelly-Betting-Part-One.html</guid>
        
        
      </item>
    
      <item>
        <title>Dissertation Construction Learnings</title>
        <description>&lt;p&gt;With my dissertation handed in it is time to reflect on what I’ve learnt in terms from constructing the 20,000 word file.&lt;/p&gt;

&lt;p&gt;Firstly, the importance of having an established workflow. If I was to calculate how my time was divided in writing my dissertation an unfortunate amount of time would have been dedicated to collating the many graphs that my work produced. I didn’t have dedicated functions outputting graphs ready to be saved into hard copies at later dates. Instead, I had a variety of functions that produced the results, but using different methods, different representations and different formats. Therefore, for my future work, every time I produce new results which leads to new graphs I need to have functions that can easily output the appropriate data when needed.&lt;/p&gt;

&lt;p&gt;Secondly, my interweaving of Tikz and LaTeX lead to a few troubles. Mainly due to the size of the graphs and lack of available memory for Latex when compiling. My previous method of importing the raw .tex files of the graphs will need to be changed such that all the .tex files are compiled into pdf’s before being imported into Latex. This will also lead to quicker compile times for my Latex document. At some points I was seeing compile times of &amp;gt;5 minutes!&lt;/p&gt;

&lt;p&gt;The use of tables and summarising the final results also lead to some troubles. Copying and pasting the results from the R output to the latex document was not the most efficient use and led to frequent updating each time the results were updated. The solution to this is slightly trickier. Perhaps an interwoven use of RMarkdown and Latex could solve this problem. But at the minute it remains unsolved.&lt;/p&gt;

&lt;p&gt;In an attempt to develop a better workflow I’ve taken to use RMarkdown to construct my research in a more verbose way. This allows me to write around code and output the results straight away without having to translate between R and latex. So far it seems to be working nicely but I’ve yet to try and output to latex.&lt;/p&gt;

&lt;p&gt;Overall, the process of writing my dissertation has shown the importance in establishing good techniques in outputting the results you find and not just focusing on getting the results. With these learnings I’ll be in good preparation for my next dissertation-like project.&lt;/p&gt;

&lt;p&gt;RMarkdown: &lt;a href=&quot;http://rmarkdown.rstudio.com/ &quot;&gt;http://rmarkdown.rstudio.com/ &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Aug 2016 00:00:00 +0100</pubDate>
        <link>/2016/08/17/Dissertation-Learnings.html</link>
        <guid isPermaLink="true">/2016/08/17/Dissertation-Learnings.html</guid>
        
        
      </item>
    
      <item>
        <title>Posterior p-values</title>
        <description>&lt;p&gt;I am now at the point in my work where I need to check my models and whether they correctly describe the data. To do this, lets introduce posterior p-values for a Bayesian model.&lt;/p&gt;

&lt;p&gt;Say we have data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; that we fit using a model &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. As we would have used MCMC to fit the model we have chain of parameter values &lt;script type=&quot;math/tex&quot;&gt;\{\theta ^{(0)} ... \theta ^{(n)}\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For each parameter value we can obtain simulated data &lt;script type=&quot;math/tex&quot;&gt;\hat{y} _i = F( \theta ^{(i)})&lt;/script&gt; such that we now have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simulated data sets.&lt;/p&gt;

&lt;p&gt;We now chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and calculate it for each simulated data set. We can now how &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; compares to the &lt;script type=&quot;math/tex&quot;&gt;T_{\text{sim}}&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; is drastically different from the simulated &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;’s then there is a problem with our model, it is not correctly picking up something intrinsic to the real data.&lt;/p&gt;

&lt;p&gt;Like all good introductions, lets add some real data to try and explain the concepts better.&lt;/p&gt;

&lt;p&gt;Our real data will be simulated from the Generalised Pareto distribution (gpd) and we will fit both an exponential model and a gpd model.&lt;/p&gt;

&lt;p&gt;So now we have three data sets, &lt;script type=&quot;math/tex&quot;&gt;y_{\text{real}}, \hat{y}_{\text{gpd}}, \hat{y}_{\text{exp}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/realandsimdata.png&quot; alt=&quot;Real and Simulated Data&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see straight away that the gpd model has nicely replicated the general shape of the real data, where as the exponential model has produced a poor fit.&lt;/p&gt;

&lt;p&gt;Now we chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. For simplicity we shall use the maximum value of the data set, &lt;script type=&quot;math/tex&quot;&gt;T(x) = \max x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we now calculate the maximum value for all our simulated datatsets of both models and see how the maximum of the real data compares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvaldist.png&quot; alt=&quot;Maximum Value Distributions&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solid black line in the maximum value of the real data set and just by inspection we can reasonably assume that the data is best modelled using a gpd model. Even more so as the x-axis is on a log scale!.&lt;/p&gt;

&lt;p&gt;So this test statistic appears to be suitable of discerning if the data comes from a gpd.&lt;/p&gt;

&lt;p&gt;Now by doing some maths you can calculate the usual power and size of the test statistic, but I’ll save that for a another blog post. This also shows how this method can seem analogous to frequentist p-values.&lt;/p&gt;

&lt;p&gt;Now, lets try using the same method but this time the real data is going to come from an exponential distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvalexp.png&quot; alt=&quot;Maximum Value Distributions Exp&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here our test statistic has failed. This is no obvious difference between the two distributions of the maximum value for the models. Therefore we can not conclude anything. A better test statistic is required!&lt;/p&gt;

&lt;p&gt;So overall, we have shown how to utilise basic test statistics and simulated datasets to analyse the suitability of a model.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&quot;&gt;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Big Red Book of Bayesian Data Analytics (Bayesian Data Analytics by Gelman et al.)&lt;/p&gt;

</description>
        <pubDate>Fri, 20 May 2016 00:00:00 +0100</pubDate>
        <link>/2016/05/20/Posterior-pvalues.html</link>
        <guid isPermaLink="true">/2016/05/20/Posterior-pvalues.html</guid>
        
        
      </item>
    
      <item>
        <title>Posterior p-values</title>
        <description>&lt;p&gt;I am now at the point in my work where I need to check my models and whether they correctly describe the data. To do this, lets introduce posterior p-values for a Bayesian model.&lt;/p&gt;

&lt;p&gt;Say we have data &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; that we fit using a model &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. As we would have used MCMC to fit the model we have chain of parameter values &lt;script type=&quot;math/tex&quot;&gt;\{\theta ^{(0)} ... \theta ^{(n)}\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For each parameter value we can obtain simulated data &lt;script type=&quot;math/tex&quot;&gt;\hat{y} _i = F( \theta ^{(i)})&lt;/script&gt; such that we now have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; somulated data sets.&lt;/p&gt;

&lt;p&gt;We now chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and calculate it for each simulated data set. We can now how &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; compares to the &lt;script type=&quot;math/tex&quot;&gt;T_{\text{sim}}&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;T_{\text{real}}&lt;/script&gt; is drastically different from the simulated &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;’s then there is a problem with our model, it is not correctly picking up something intrinsic to the real data.&lt;/p&gt;

&lt;p&gt;Like all good introductions, lets add some real data to try and explain the concepts better.&lt;/p&gt;

&lt;p&gt;Our real data will be simulated from the Generalised Paero distribution (gpd) and we will fit both an exponetial model and a gpd model.&lt;/p&gt;

&lt;p&gt;So now we have three data sets, &lt;script type=&quot;math/tex&quot;&gt;y_{\text{real}}, \hat{y}_{\text{gpd}}, \hat{y}_{\text{exp}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/realandsimdata.png&quot; alt=&quot;Real and Simulated Data&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see straight away that the gpd model has nicely replicated the general shape of the real data, where as the exponential model has produced a poor fit.&lt;/p&gt;

&lt;p&gt;Now we chose a test statistic, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. For simplicity we shall use the maximum value of the data set, &lt;script type=&quot;math/tex&quot;&gt;T(x) = \max x_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we now calculate the maximum value for all our simulated datatsets of both models and see how the maximum of the real data compares.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvaldist.png&quot; alt=&quot;Maximum Value Distributions&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solid black line in the maximum value of the real data set and just by inspection we can reasonably assume that the data is best modelled using a gpd model. Even more so as the x-axis is on a log scale!.&lt;/p&gt;

&lt;p&gt;So this test statistic appears to be suitable of discerning if the data comes from a gpd.&lt;/p&gt;

&lt;p&gt;Now by doing some maths you can calculate the usual power and size of the test statistic, but I’ll save that for a another blog post. This also shows how this method can seem anallagous to frequantist p-values.&lt;/p&gt;

&lt;p&gt;Now, lets try using the same method but this time the real data is going to come from an exponential distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/maxvalexp.png&quot; alt=&quot;Maximum Value Distributions Exp&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here our test statistic has failed. This is no obvious difference between the two distributions of the maximum value for the models. Therefore we can not conclude anything. A better test statistic is required!&lt;/p&gt;

&lt;p&gt;So overall, we have shown how to utilise basic test statistics and simulated datasets to analyse the suitability of a model.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&quot;&gt;http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Big Red Book of Bayesian Data Analytics (Bayesian Data Analytics by Gelman et al.)&lt;/p&gt;

</description>
        <pubDate>Fri, 20 May 2016 00:00:00 +0100</pubDate>
        <link>/2016/05/20/PosteriorPValues.html</link>
        <guid isPermaLink="true">/2016/05/20/PosteriorPValues.html</guid>
        
        
      </item>
    
      <item>
        <title>Bulk Downloading from Turnitin using Python.</title>
        <description>&lt;p&gt;As a teaching assistant, occasionally I get assigned to marks a series of papers. This involves tediously searching for the students paper on the Turnitin app inside moodle before clicking a download button. When you’ve got 36 papers to download, this is far to much clicking and mouse movement. So I wrote a Python script to automate it.&lt;/p&gt;

&lt;p&gt;Firstly, I had to consider how the file was pulled from the server. Thankfully, it was a simple POST request with the paper id as one of the parameters. Using the FireFox addon Tamper, I was able to easily view and submit a custom post request. All it required was a session id and paper id.&lt;/p&gt;

&lt;p&gt;Moving onto Python, I used the urllib2 package to open the custom POST url. Then it was a case of writing the response to a pdf file. Extending this to 36 urls is as simple as looping through each line in a file.&lt;/p&gt;

&lt;p&gt;In Python-esque pseudo-code, this looks like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urllib2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urlopen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sessionid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paperid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pdf_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The simplicity of the urllib2 is what makes this short script so easy to construct and use.&lt;/p&gt;

&lt;p&gt;Future work would be to get the session id automatically rather than manually copying and pasting it in.&lt;/p&gt;

&lt;p&gt;On an unrelated note, looks like I need to fix the code formatting above. I’ll save that for another day.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/28/Python-Url.html</link>
        <guid isPermaLink="true">/2016/01/28/Python-Url.html</guid>
        
        
      </item>
    
      <item>
        <title>Films I&#39;ve Watched In January 2016</title>
        <description>&lt;p&gt;Ordered as per my memory!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;V for Vendetta&lt;/li&gt;
  &lt;li&gt;Cartel Land&lt;/li&gt;
  &lt;li&gt;Howl&lt;/li&gt;
  &lt;li&gt;Silence of The Lambs&lt;/li&gt;
  &lt;li&gt;Sicario&lt;/li&gt;
  &lt;li&gt;Star Wars 7&lt;/li&gt;
  &lt;li&gt;Legend&lt;/li&gt;
  &lt;li&gt;Mad Max: Fury Road&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Silence of the Lambs easily comes out top and I’ll be the millionth person to say that the acting and story is captivating. If you’ve not seen it, put it high on your list of things to watch.&lt;/p&gt;

&lt;p&gt;Mad Max is the most over rated. Despite the various internet list raving about it the non-stop action and lack of real character development just didn’t click with me. The fact that they ended up turning around on the road just to go back again felt like a bit of a cop out.&lt;/p&gt;

&lt;p&gt;Legend was another underwhelming occurrence. I felt like it was trying to appeal to the separate male and female audiences in tandem and not really connecting with either. Whilst the technical effect of having Tom Hardy play both roles was interesting, it wasn’t enough to salvage the story. In the end it was the worst of both a love story and a gangster film.&lt;/p&gt;

&lt;p&gt;Similarly, Sicario started off as a cop/military film before throwing that all out the window to become a revenge film. The first half is intense and lets you develop a real distrust of the authority figures, but this eventually fizzles out as the film moves onto the revenge part before ending with either questions for the audience to think about themselves but they actually feel like plot holes. Still, it peaked my interest in the Mexican Cartel story, hence I gave Cartel Land a watch. It’s an alright documentary, but less about the Cartels and more about the resistance against them.&lt;/p&gt;

&lt;p&gt;My only trip to the cinema in January was to beat the crowds and finally see the behemoth that is the new Star Wars. It does everything right but it doesn’t push any boundaries. The more I think of it, the more I realise that it is basically a HD remake of A New Hope. But still, I look forward to number 8 and see whether that is a bit more adventurous.&lt;/p&gt;

&lt;p&gt;Finally, Howl, your classic lower budget horror film. It ticks the boxes and provides enough core and creature shots to leave you feeling satisfied. Watch it as an appetiser to Dog Soldiers.&lt;/p&gt;

</description>
        <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
        <link>/2016/01/25/January-Films.html</link>
        <guid isPermaLink="true">/2016/01/25/January-Films.html</guid>
        
        
      </item>
    
  </channel>
</rss>
