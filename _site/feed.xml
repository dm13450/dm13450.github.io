<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dean Markwick</title>
    <description>All rights reserved.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 26 Jan 2017 09:16:14 +0000</pubDate>
    <lastBuildDate>Thu, 26 Jan 2017 09:16:14 +0000</lastBuildDate>
    <generator>Jekyll v3.1.0</generator>
    
      <item>
        <title>Inhomogenous Poisson Process</title>
        <description>&lt;p&gt;My first forray into statistics and finance begins with the simulation, fitting and checking of the inhomogenous poisson process.
THe inhomogenous poisson process is easily defined, instead of constant rate $\lambda$ as per the usual Poisson process, the rate can now varry in time.&lt;/p&gt;

&lt;p&gt;Simulating this is done using a method called thinning; a Poisson process is simulated with rate greater than the inhomogenous rate, with points being rejected, or “thinned” out to give the final inhomogenous process.&lt;/p&gt;

&lt;p&gt;Therefore the algorithm runs as follows: 
1. Generate a Poisson varaible with rate $\lambda ^&lt;em&gt;$ such that $\lambda ^&lt;/em&gt; &amp;gt; \lambda (t) \forall t$. Increment time by this variable. 
2. Calculate the ratio $\lambda (t) / \lambda ^*$ and compare this to a unifromally distributed variable. If the ratio is greater then accept the time else reject the time and continue with step one again. 
3. The accepted times are the correct times of the inhomogenous process.&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jan 2017 09:16:14 +0000</pubDate>
        <link>/2017/01/26/InhomoPoisson.html</link>
        <guid isPermaLink="true">/2017/01/26/InhomoPoisson.html</guid>
        
        
      </item>
    
      <item>
        <title>Febfilms</title>
        <description>&lt;hr /&gt;
&lt;p&gt;layout: post
title: Films and Books of February
date: 2016-02-03
***&lt;/p&gt;

&lt;p&gt;Carrying from last months post and an in an attempt to make a habit out of writing I am going to have a look at the films and books consumed in February. 
This time I’ve ordered them roughly on how good they are.&lt;/p&gt;

&lt;p&gt;** Films&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spotlight&lt;/li&gt;
  &lt;li&gt;The Big Short&lt;/li&gt;
  &lt;li&gt;What We Do In The Shadows&lt;/li&gt;
  &lt;li&gt;Edge of Tomorrow&lt;/li&gt;
  &lt;li&gt;Alien and Aliens&lt;/li&gt;
  &lt;li&gt;Bridge of Spies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spotlight and The Big Short are going to find themselves tough to beat in 2016. Both stellar films highlighting the state of corruption in our society albeit with different tones.&lt;/p&gt;

&lt;p&gt;Moving onto the more lighthearted films, What We Do In The Shadows (WWDITS) is an original, low budgety comedy that will make you laugh. It takes on the mockumentary approach adds a supernatural spin. It takes all the good parts from The Inbetweeners, The Office and vampire lore and package them into a film. This is what WWDITS essentially is. I look forward to a sequal and more from the director. 
Speaking again of originallity, Edge of Tomorrow will take any viewer back to there childhood. It’s got robots, aliens, high tech weaponary and a respawning mechanic. If you want a film to watch on a saturday night after the X-Factor has finished you’d be hard pressed to find something as enjoyable as Edge of Tomorrow.&lt;/p&gt;

&lt;p&gt;Bridge of Spies fell a bit to me. Like last months Legend viewing, Bridge of Spies couldn’t deciede whether it wanted to be a courtroom drama or more focused on the negotiation aspect between East Germany and Russia. As a result you got two unsatisfactory parts of the film. It also had the usual Spielberg quirks: a moment of two people talking over each other, the main character going from being hated to being loved as if public opinion is that black and white. By all means it is a good film, but nothing more. Watch it once and continue with your life.&lt;/p&gt;

&lt;p&gt;Finally, inspired by a play-through of Alien: Isolation on the PS4 I decieded to eduacte myself in the actual films. Alien had the very ‘classical’ sci-fi feel to it, with the beeping terminals, robots so realistic they pass of as humans and finally actual people in rubber suits opposed to CGI. Watching it takes you into that world and makes you nostalgic for the alternate timeline where computers fill rooms and sleeping pods. The influence it has on cinema is easily digested and thankfully doesn’t move into the ‘Seinfield is unfunny’ trope. Similarly, Aliens takes the same angle, although much less delicate. There are even paralels to Edge of Tomorrow and Aliens.&lt;/p&gt;

&lt;p&gt;** Books&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Boys&lt;/li&gt;
  &lt;li&gt;Chris Hadfield’s Autobiography&lt;/li&gt;
  &lt;li&gt;The Art of Deception&lt;/li&gt;
  &lt;li&gt;Crytopnomicon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout January I was a slave to the behomoth that is Cryprtonomicon and I’m ashamed to say that I still haven’t finished it. I’m 600 pages in and still feel like the proper story is yet to start. Everynow and then one of the characters goes on an arc that has me turning pages, but that usually finishes at the end of the chapter and then the book loses momentum and I find myself looking at the back trying to work out how many pages I need to read a day to finish before the next leap year. For this reason I think I’m going to have to relegate Crytonomicon to the unfinished pile.&lt;/p&gt;

&lt;p&gt;Moving onto the non-fiction books it has been a mixture of finance, space and security. 
With the debate of IEX continually appearing on my Twitter timeline and my Phd work touching it’s toes in microstructure, Flash Boys has been on my to-read list for a while. I’m still strugling to form an opinion on the morality of HFT even after reading it. I feel like the majority of what is described in Flash Boys can be categorised as market impact. If you place a large order, other traders are going to adjust there prices. The finite nature of passing messages back and forth from exchanges means that there will be information asymetry and thus arbitrage oppurunities.&lt;/p&gt;

&lt;p&gt;From low latency to high latency, Chris Hadfield’s book is a simple and consice account of his time training and actual time in space on various missions. I enjoyed it for the insight into the organisation of NASA and how as a Canadian there is a slightly different path to becoming a spaceman. I imagine it’s similar to the UK and becoming part of the ESA crew. 
Whilst the book is an easy read, I found it a bit light on the details and prehaps a bit of negativity to give it a bit of balance. It felt much like a good PR read for NASA and many of it’s weaknesses were actually it’s strenghts. I.e. the flat organisational structure that means that returning astronaughts are placed bottom of the pile. Much like Alex Fergusan said, no player bigger than the club - no astronaught bigger than NASA.&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jan 2017 09:16:14 +0000</pubDate>
        <link>/2017/01/26/FebFilms.html</link>
        <guid isPermaLink="true">/2017/01/26/FebFilms.html</guid>
        
        
      </item>
    
      <item>
        <title>An Introduction to Julia and Distributions</title>
        <description>&lt;p&gt;Julia is a new language on the block aimed at being a suitable mid point between the adaptability of Python and the speed of Matlab. Its a nice fall-back when my R code is just that bit too slow to really churn through some numbers.&lt;/p&gt;

&lt;p&gt;On of the main benefits of using R is the ease at which the ‘standard’ distributions are available. Want exponentially distributed random variables? Just call rexp()! Want the pdf of the gamma distribution? dgamma() is there to help you. With Julia this type of functionality is in the &lt;a href=&quot;https://github.com/JuliaStats/Distributions.jl&quot;&gt;Distribution module&lt;/a&gt;, so takes just a little bit more of work to get the same functionality.&lt;/p&gt;

&lt;p&gt;In this post I will outline how the basics of the distributions package and how you can replicate some of the functionality of R.&lt;/p&gt;

&lt;p&gt;Firstly, we need to install the Distributions package. This is done by calling &lt;code class=&quot;highlighter-rouge&quot;&gt;Pkg.add(&quot;Distributions&quot;)&lt;/code&gt;. Now that is installed we need to load it into the namespace. Open a new Julia instance and load the package with &lt;code class=&quot;highlighter-rouge&quot;&gt;using Distributions&lt;/code&gt;. The necessary functions are now loaded.&lt;/p&gt;

&lt;p&gt;Our first exercise will be to sample &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; exponentially distributed variables and check that the density of the samples tends to the pdf of the exponential distribution as &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; becomes larger. 
The first step in this code is to define our distribution. As the exponential distribution only requires one parameter, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, this is as simple as calling &lt;code class=&quot;highlighter-rouge&quot;&gt;Exponential(m)&lt;/code&gt; in our code. Now we use a number of different functions on the distribution.&lt;/p&gt;

&lt;p&gt;We can sample from this distribution using &lt;code class=&quot;highlighter-rouge&quot;&gt;rand(dist, N)&lt;/code&gt; where &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; is the number of samples to draw. We can then overlay the pdf of the distribution by using &lt;code class=&quot;highlighter-rouge&quot;&gt;pdf(dist, x)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Combing these commands allows us to draw a graph (using the Julia package &lt;a href=&quot;http://gadflyjl.org/stable/&quot;&gt;Gadfly&lt;/a&gt;) like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/expPlot.svg&quot; alt=&quot;Exponential Plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we can see the small sample size does not resemble the pdf but the large sample size does. So we are correctly drawing from the exponential distribution as expected.&lt;/p&gt;

&lt;p&gt;There are also other functions available. A great example is calculating the mean of a log-normal distribution. This distribution is defined with two parameters; &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s^2&lt;/script&gt;. However, the mean of the distribution is not equal to &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;. Instead it is &lt;script type=&quot;math/tex&quot;&gt;\exp(m+\frac{s^2}{2})&lt;/script&gt;. The Distributions package in Julia knows this. So by simply calling &lt;code class=&quot;highlighter-rouge&quot;&gt;mean&lt;/code&gt; on the &lt;code class=&quot;highlighter-rouge&quot;&gt;LogNormal()&lt;/code&gt; object you can return the theoretical mean of the distribution and not have to worry about the parametrisation specifics of the distribution.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dist = LogNormal(1,4)
mean(dist) == exp(1 + 4^2/2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Overall, Julia and the Distributions package offer similar functionality to R. You can easily replicate some of the functions in R with very effort in Julia. This can be a useful tool if R is not quite cutting it on the speed front.&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
        <link>/2017/01/26/JuliaDistributions.html</link>
        <guid isPermaLink="true">/2017/01/26/JuliaDistributions.html</guid>
        
        
      </item>
    
      <item>
        <title>Mike Dean and the Ref Radar</title>
        <description>&lt;p&gt;It was a tough day at the office last week for Mike Dean. The match was Manchester United vs West Ham and after 15 minutes he dismissed Feghouli for a foul tackle. The tempo of the game changed. West Ham were on the back foot and eventually conceded two goals to Man Utd. Undeniably the red card had a big effect on the outcome of the game. It has since been rescinded, showing that the FA agree with fans that perhaps it should not have been a red card.&lt;/p&gt;

&lt;p&gt;There has also been a tweet circulating that Mike Dean in his last 7 Tottenham games has sent off three opposition players. On first reading this raises some eyebrows. Is this just a statistical oddity or is there something deeper in the numbers? A prime opportunity to explore our RefRadar of Mike Dean and see how he compares to the other Premier League referees.&lt;/p&gt;

&lt;p&gt;Lets look at the season so far (2016/17) in isolation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadar.png&quot; alt=&quot;Mike Dean Ref Radar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So as we can see here, Mike Dean does like penalising the away team more than the home team for both fouls and yellow cards. Obviously these two stats are going to be correlated. But it is still interesting to see how he is fairly close to the average amount for penalising the home team, but for the away team he is frequently giving fouls and yellow cards out.&lt;/p&gt;

&lt;p&gt;Mike Dean has officiated the 5th most amount of games in the league. So let’s look at the 3 other referees who have had more games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadarComp.png&quot; alt=&quot;Mike Dean Ref Radar Comp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most notably, three out of four of the most active referees this season are the ones giving the most yellow cards to the away teams whereas they are middle of the pack for the home yellow cards. A similar trend is seen for the away fouls, but like previous, these two variables are going to be highly correlated.
The stand out statistic though is Mike Deans average away red cards. He is a clearly the most likely to send off a player of the away team out of this selection of referees. In fact the three other referees haven’t sent anyone off from the away team yet this season! But something to keep in mind is that this is a really small sample size. The season is about half way through so there is still plenty of time for the other referees to catch up on the red cards.&lt;/p&gt;

&lt;p&gt;So in conclusion, whatever match Mike Dean is appointed to next, the away team should keep in my mind a red card is never far away. But perhaps the FA are going to have a word with Mike Dean and he will have an easy couple of games to bring his average down. The RefRadar here has been useful to look at how active the officials have been compared to their peers. Lets see what the next round of football brings!&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
        <link>/2017/01/07/Mike-Dean-Ref-Radar.html</link>
        <guid isPermaLink="true">/2017/01/07/Mike-Dean-Ref-Radar.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing the Referee Radar</title>
        <description>&lt;p&gt;The radar plot is a good way to analyse different metrics across a group. The football player radar made popular by &lt;a href=&quot;http://statsbomb.com/2016/04/understand-football-radars-for-mugs-and-muggles/&quot;&gt;Ted Knutson&lt;/a&gt; is good at comparing players and seeing how their stats stack up against one another. Here I will be taking a similar concept and using the radar plot to analyse the referee’s in the professional game in England.&lt;/p&gt;

&lt;p&gt;First, we need to chose some metrics. Using the data from &lt;a href=&quot;http://www.football-data.co.uk/&quot;&gt;Football-Data&lt;/a&gt; we can download all the league matches from the last three seasons. In this data we are privy to the number of fouls, yellow cards and red cards for both the home and away team. That gives us 6 variables for each match and each referee, the perfect amount for a radar plot.&lt;/p&gt;

&lt;p&gt;For each referee we can calculate the average amount of fouls, yellow and red cards the gave to both the home and away team. This will allow us to detect whether a referee is particularly card happy or even has a home or away bias. In terms of practicality, we have to set a threshold for minimum number of games officiated. We remove any ref that has refereed less than 20 games over the three seasons.&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://github.com/ricardo-bion/ggradar&quot;&gt;ggradar&lt;/a&gt; package  and tweaking some of the graphical parameters we are able to come up with the following plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/RefereeRadar.png&quot; alt=&quot;RefereeRadar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we are comparing four referees (at random) and how their metrics personally match against the population of all referees. Here we can see Andy D’Urso is particular for sending an away player off. Simon Hooper is quite a stickler for a foul. Nigel Miller is laid back, not giving many fouls and not giving out the yellow cards either. Chris Kavanagh is very middle of the pack, consistent across home and away for both fouls and cautions.&lt;/p&gt;

&lt;p&gt;The package &lt;code class=&quot;highlighter-rouge&quot;&gt;ggradar&lt;/code&gt; requires all the variables to have a consistent range. For this I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;rescale&lt;/code&gt; function of R to remap the averages to &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;. Therefore the actual values of the metrics are lost in the radar plot. This is something I’ll work on to include in future versions. I’ll also be creating an app, either in shiny or JavaScript that will allow users to compare different referees as they see fit.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/24/Ref-Radar-Intro.html</link>
        <guid isPermaLink="true">/2016/12/24/Ref-Radar-Intro.html</guid>
        
        
      </item>
    
      <item>
        <title>The Data Dialogue - At War with Data</title>
        <description>&lt;p&gt;On Wednesday I took a stroll down to King’s College across the river for a seminar on data and warfare. A bit of a broad subject for a conference. The seminar speakers came from a variety of fields where the innovative usage of data is less obvious and more unique.&lt;/p&gt;

&lt;p&gt;The first talk was by &lt;a href=&quot;http://www.kcl.ac.uk/hr/diversity/meettheprofessors/artshums/croueche.aspx&quot;&gt;Charlotte Rouche&lt;/a&gt; of the Kings college archaeological department. She highlighted how maps, both ancient and current, play an important role in her archaeological research. Understanding how the land is divided and even the names of places in various languages gives an important insight into how civilisation has evolved throughout the ages. Mapping data is now ubiquitous with the likes of Google Maps and Google Earth and this can be useful in observing how areas have developed over time. The aerial photographs of sites of archaeological interest can be periodically monitored for change and help with preservation - fighting the war against decay. However, when cataloguing such data Prof. Rouche raised the importance question of accessibility . Should the public be allowed to see aerial photographs and accurate co-ordinates of culturally and theologically significant monuments? In times of war, this could potentially be a shopping list for attacks. This shows how perhaps a fully open data source is not always the correct answer.&lt;/p&gt;

&lt;p&gt;The second talk was by &lt;a href=&quot;https://www.ucl.ac.uk/spacetimelab/people/kate-bowers&quot;&gt;Kate Bowers&lt;/a&gt; of the Crime Science department at UCL. Her talk was on the usage of police data, which has similar disclosure properties to the Rouche’s map data. Prof. Bowers showed how the GPS data collected from police cars can provide maps of frequent patrol paths of the Metropolitan police. Combining this with 999 call monitoring she was able to give an indication of where there was deficiency in policing were arising. This can lead to policy implementation and a more efficient patrol path for police officers. In this case the war on crime was being fought with this new data.&lt;/p&gt;

&lt;p&gt;The third talk was by &lt;a href=&quot;https://kclpure.kcl.ac.uk/portal/robert.stewart.html&quot;&gt;Robert Stewart&lt;/a&gt;. He is a mental health doctor from the NHS and Professor at King’s. Interestingly, his trust has developed a searchable, anonymous database for mental health researchers. This allowed them to query various parameters across patient groups. A very interesting research tool that required individual patient consent but once granted provides researchers with large amounts of real world clinical data. Its success shows that it is possible to use the health data of patients anonymously without infringing on privacy issues. Currently it is being used to help study drug safety and other questions that required long term monitoring.  Now I have previously worked for a NHS trust in the IT department and have an understanding of different software systems available for the NHS and the various data sharing regulations in place. Therefore this talk was a good insight into how other trusts can conquer some of the data sharing rules and how researchers can get access to such data. As Prof. Stewart is a mental health doctor his work can be framed as the war against mental afflictions.&lt;/p&gt;

&lt;p&gt;The final talk was based on finding adversaries on the internet and looking at how such actors can make themselves known unwillingly. This talk was given by &lt;a href=&quot;http://www.kcl.ac.uk/sspp/departments/warstudies/people/professors/rid.aspx&quot;&gt;Thomas Rid&lt;/a&gt; from the War Studies department at King’s. He offered the example of the Poseta email &lt;a href=&quot;https://en.wikipedia.org/wiki/Podesta_emails&quot;&gt;leak&lt;/a&gt;. By using the classic “reset you password here” phishing scam, Podesta clicked on a bitly link that populated a fake web page with his gmail information and required him to submit his password to change it. Instead, the changing the password button just forwarded his password to the adversary. We know that this was the vector chosen by the attackers due to the public nature of the bitly account. Whoever set the account up forgot to set the profile to private. This led to the account being traced back  to some actor. Who it actually was is up to speculation at the time of writing.&lt;/p&gt;

&lt;p&gt;Overall, it was an enjoyable conference. I learnt about some new domains and in a more focused and “apply-side” way. It was less about algorithm run times and more about what the algorithms actually do and how they can be used.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/07/DataDialogue.html</link>
        <guid isPermaLink="true">/2016/12/07/DataDialogue.html</guid>
        
        
      </item>
    
      <item>
        <title>Notes from a Quantcast Talk</title>
        <description>&lt;p&gt;As I sit on the train to London waiting for my model to finish fitting I realised that it had been a while since I had written a blog post.&lt;/p&gt;

&lt;p&gt;My PhD school holds a bimonthly seminar and this weeks guest speaker was Dr. Peter Day who is a director of engineering at &lt;a href=&quot;https://www.quantcast.com&quot;&gt;Quantcast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Quantcast is a 21st century advertising agency offering real time advertisement strategy and analytic tools. The lecture was based around their use of data and how statistics and infrastructure has helped make advertising more relevant and serve better adds.&lt;/p&gt;

&lt;p&gt;My first &lt;code class=&quot;highlighter-rouge&quot;&gt;did you know&lt;/code&gt; was that the adverts you see on websites are sold in real time as you load the page. So in the time it takes from clicking a link on Goole to the page appearing in your browser, the add space has been bought microseconds earlier and now serving a specific add designed for you. A remarkable engineering achievement, that in the short time it takes from clicking a link to seeing a page that an auction takes place, a winner is found and the add is served. But this is not the main product of Quantcast.&lt;/p&gt;

&lt;p&gt;Instead, Quantcast is more about finding out more about population behaviours and how effective that advert will be. They crunch the necessary data from the variety of cookies they collect and come up with a pricing strategy based on who might see the add. Where as one company might try to show the add to as many people as possible (the fire-hose strategy) Quantcast might drill down on certain factors and change their bidding price based on these factors.&lt;/p&gt;

&lt;p&gt;This problem is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;big data&lt;/code&gt; problem. The amount of data you can collect from cookies; visited websites, location, device etc. can lead to many other inferences which gives a large amount of variables for a large amount of people. Computing this information lead to Quantcast developing their own database and data tools as the current market leaders (Hadoop and derivatives) where not performing well enough. Although they did hint that they was looking at moving to AWS to solve some of their infrastructure problems.&lt;/p&gt;

&lt;p&gt;Overall, it was an interesting talk about a field that I hadn’t really paid that much attention too previously and I definitely learnt something new. It also made me thankful of my add blocker and slightly more paranoid. So thank to Quantcast for coming down and delivering an enjoyable seminar!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/02/Quantcast.html</link>
        <guid isPermaLink="true">/2016/12/02/Quantcast.html</guid>
        
        
      </item>
    
      <item>
        <title>Notes from a Quantcast Talk</title>
        <description>&lt;p&gt;As I sit on the train to London waiting for my model to finish fitting I realised that it had been a while since I had written a blog post.&lt;/p&gt;

&lt;p&gt;My PhD school holds a bimonthly seminar and this weeks guest speaker was Dr. Peter Day who is a director of engineering at &lt;a href=&quot;www.quantcast.com&quot;&gt;Quantcast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Quantcast is a 21st century advertising agency offering real time advertisement strategy and analytic tools. The lecture was based around their use of data and how statistics and infrastructure has helped make advertising more relevant and serve better adds.&lt;/p&gt;

&lt;p&gt;My first &lt;code class=&quot;highlighter-rouge&quot;&gt;did you know&lt;/code&gt; was that the adverts you see on websites are sold in real time as you load the page. So in the time it takes from clicking a link on Goole to the page appearing in your browser, the add space has been bought microseconds earlier and now serving a specific add designed for you. A remarkable engineering achievement, that in the short time it takes from clicking a link to seeing a page that an auction takes place, a winner is found and the add is served. But this is not the main product of Quantcast.&lt;/p&gt;

&lt;p&gt;Instead, Quantcast is more about finding out more about population behaviours and how effective that advert will be. They crunch the necessary data from the variety of cookies they collect and come up with a pricing strategy based on who might see the add. Where as one company might try to show the add to as many people as possible (the fire-hose strategy) Quantcast might drill down on certain factors and change their bidding price based on these factors.&lt;/p&gt;

&lt;p&gt;This problem is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;big data&lt;/code&gt; problem. The amount of data you can collect from cookies; visited websites, location, device etc. can lead to many other inferences which gives a large amount of variables for a large amount of people. Computing this information lead to Quantcast developing their own database and data tools as the current market leaders (Hadoop and derivatives) where not performing well enough. Although they did hint that they was looking at moving to AWS to solve some of their infrastructure problems.&lt;/p&gt;

&lt;p&gt;Overall, it was an interesting talk about a field that I hadn’t really paid that much attention too previously and I definitely learnt something new. It also made me thankful of my add blocker and slightly more paranoid. So thank to Quantcast for coming down and delivering an enjoyable seminar!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/02/QuantCast.html</link>
        <guid isPermaLink="true">/2016/12/02/QuantCast.html</guid>
        
        
      </item>
    
      <item>
        <title>Thoughts on Weights in Bayesian Regression</title>
        <description>&lt;p&gt;Weighted regression is common in frequentist approaches to regression. Typically, the weights are known and can be interpreted as the amount each data point is allowed to influence the link between the variables in question. This can be written as
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \sum _i w_i \beta _i x_i&lt;/script&gt; 
where &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; is the ‘weighting’.&lt;/p&gt;

&lt;p&gt;For example if you have some knowledge that a particular data point is less accurate than all the others. You can assign it a weight such that relative to all the other data points it has less of an impact on the final result. Weighted regression can also be a useful tool in building robust regression models - ones that are less susceptible to outliers as such outliers are given small weights.&lt;/p&gt;

&lt;p&gt;However, when it comes to adding in weights in a Bayesian manner for a regression problem the intuition falls apart.&lt;/p&gt;

&lt;p&gt;Weights in their nature imply that further information is known about the data and the model that it came from. This is a violation of Bayesian thinking as we are no longer considering the sample of data fixed but now dependent on some other function that is manifested in the weights. This weighting function in turn can only be found once we have observed all the data.&lt;/p&gt;

&lt;p&gt;Then by specifying a weighting function we are changing our posterior to be closer to some ideal distribution rather than just letting the data speak for itself. Essentially, changing the results to be closer to what we think the answer should be. Obviously this is not a good practise for any data analysis so we are forced to conclude that weights are not intuitive in a Bayesian way. Instead, if we think that there is a factor influencing the model we should include the factor as a parameter and assess its influence.&lt;/p&gt;

&lt;p&gt;Overall, the fact that weight cannot be thought of as Bayesian is cool little thought experiment and goes to show the divergence between frequentist and Bayesian approaches.&lt;/p&gt;

&lt;p&gt;The man himself, Andrew Gelman, discusses the issue of weighted regression and Bayesian thinking &lt;a href=&quot;https://groups.google.com/forum/#!msg/stan-dev/5pJdH72hoM8/GLW_mTeaObAJ&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 27 Oct 2016 00:00:00 +0100</pubDate>
        <link>/2016/10/27/Weighted-Bayes.html</link>
        <guid isPermaLink="true">/2016/10/27/Weighted-Bayes.html</guid>
        
        
      </item>
    
      <item>
        <title>Kelly Betting - Part Two</title>
        <description>&lt;p&gt;In my previous post I have outlined the basics of Kelly betting. Now I will be looking at the optimal bet size for placing bets on multiple simultaneous events that are independent of one another. We will be using R to numerically solve the resulting equations and hopefully learn some quirks of function optimisation in R.&lt;/p&gt;

&lt;p&gt;Again this requires maximising the expected value of the log of the bankroll&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[ \log (x)\right] = \sum _i p_i \log (1+ (b_i-1) x_i) + (1-p_i) \log (1- x_i)&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;where each event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; of occurring, decimal odds of &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is the size of the bet.&lt;/p&gt;

&lt;p&gt;Now that we have multiple bets, the total amount staked must be less than 1&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum _i x_i \leq 1&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;however in practise this is usually capped at some lesser value which is then referred to as fractional Kelly betting.&lt;/p&gt;

&lt;p&gt;Now solving this sum of bets is possible analytically but it is not the easiest nor instructive. Instead, lets turn to maximising the expectation numerically. For this, we turn to R and its optim function.&lt;/p&gt;

&lt;p&gt;Firstly, let us define our expectation function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;due to the vectorised nature of R functions both &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; can be lists and there is no need to loop through each value.&lt;/p&gt;

&lt;p&gt;To find the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values for given &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; values that maximise the bank roll we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;optim&lt;/code&gt; function. &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html&quot;&gt;Optim&lt;/a&gt; is R’s numerical optimisation routine that can implement a number of different algorithms for finding the minimum of a given function. Therefore, for it to be any use to us, we need to multiply our function by -1 such that the maximum now becomes the minimum.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p = c(0.7, 0.8)
b = c(1.3, 1.2)
optim(c(0.5, 0.5), function(x) (-1)*expectedBankRoll(x, p, b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code will find the two &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; values that maximise the expected bank roll and therefore consider the output as the Kelly bet for two simultaneous results.&lt;/p&gt;

&lt;p&gt;But there is a few caveats. Firstly we need to account for the fact that the sum of our bets must be less than 1. Secondly, the bets must also be positive numbers. To account for these restrictions we must modify our expected bank roll function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;expectedBankRoll &amp;lt;- function(x, p, b){
  if(sum(x) &amp;gt; 1 | any(x&amp;lt;0)){return(-99999)}
  expBankRoll = p*log(1+(b-1)*x) + (1-p)*log(1-x)
  return(sum(expBankRoll))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The returning of a large value if any of the restrictions of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; are broken ensures that we get reasonable results from optim. This also has the added benefit of setting &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; to zero for any event that does not have a positive expected value based on the odds offered. Therefore, to arrive at the optimal bet sizes for a collection of events, just pass in the probabilities and the odds.&lt;/p&gt;

&lt;p&gt;In the next part I will be looking at bet hedging and how this can effect the stake size and overall profitability of a betting system.&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Sep 2016 00:00:00 +0100</pubDate>
        <link>/2016/09/29/Kelly-Betting-Part-Two.html</link>
        <guid isPermaLink="true">/2016/09/29/Kelly-Betting-Part-Two.html</guid>
        
        
      </item>
    
  </channel>
</rss>
