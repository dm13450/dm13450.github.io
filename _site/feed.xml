<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dean Markwick</title>
    <description>All rights reserved.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 09 Jun 2017 17:55:25 +0100</pubDate>
    <lastBuildDate>Fri, 09 Jun 2017 17:55:25 +0100</lastBuildDate>
    <generator>Jekyll v3.1.0</generator>
    
      <item>
        <title>Gaussian Processes In Julia</title>
        <description>&lt;p&gt;Gaussian processes are a way of modelling.&lt;/p&gt;

&lt;p&gt;In a very simple example, lets generate some data from a 2D Gaussian with zero mean and unit variance. 
If we plot this with both dimmensions on either axis we get the familiar scatter plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gp2d_simple.svg&quot; alt=&quot;2D Normal Distribtuion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But if rethink the dimmensions as categories on the x-axis, we flip the point to two sperate groups.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gp2d_split.svg&quot; alt=&quot;2D Split&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each point is connected by its corrosponding point in the next dimmension. We build up a connection between the two dimmensions based on the correlation between each dimmension. We know that the coupling between the two dimmensions in two dimmensional Gaussian is controlled by the off diagonal elements in the variance matrix, which in the above example are zero. What happens if we include some correlation?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gp2d_split_cor.svg&quot; alt=&quot;2D Split Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here the variance matrix has -0.8 on the off diagonals and 1 on the diagnoals. So each point in the first direction is negativly correlated with the second dimmesnsion. This is easily observed in this graph, the higher points in the first dimmension lead to the lower points in the next dimmension and vice-versa.&lt;/p&gt;

</description>
        <pubDate>Fri, 09 Jun 2017 17:55:25 +0100</pubDate>
        <link>/2017/06/09/gaussianprocess.html</link>
        <guid isPermaLink="true">/2017/06/09/gaussianprocess.html</guid>
        
        
      </item>
    
      <item>
        <title>Inhomogenous Poisson Process</title>
        <description>&lt;p&gt;My first forray into statistics and finance begins with the simulation, fitting and checking of the inhomogenous poisson process.
THe inhomogenous poisson process is easily defined, instead of constant rate $\lambda$ as per the usual Poisson process, the rate can now varry in time.&lt;/p&gt;

&lt;p&gt;Simulating this is done using a method called thinning; a Poisson process is simulated with rate greater than the inhomogenous rate, with points being rejected, or “thinned” out to give the final inhomogenous process.&lt;/p&gt;

&lt;p&gt;Therefore the algorithm runs as follows: 
1. Generate a Poisson varaible with rate $\lambda ^&lt;em&gt;$ such that $\lambda ^&lt;/em&gt; &amp;gt; \lambda (t) \forall t$. Increment time by this variable. 
2. Calculate the ratio $\lambda (t) / \lambda ^*$ and compare this to a unifromally distributed variable. If the ratio is greater then accept the time else reject the time and continue with step one again. 
3. The accepted times are the correct times of the inhomogenous process.&lt;/p&gt;

</description>
        <pubDate>Fri, 09 Jun 2017 17:55:25 +0100</pubDate>
        <link>/2017/06/09/InhomoPoisson.html</link>
        <guid isPermaLink="true">/2017/06/09/InhomoPoisson.html</guid>
        
        
      </item>
    
      <item>
        <title>Febfilms</title>
        <description>&lt;hr /&gt;
&lt;p&gt;layout: post
title: Films and Books of February
date: 2016-02-03
***&lt;/p&gt;

&lt;p&gt;Carrying from last months post and an in an attempt to make a habit out of writing I am going to have a look at the films and books consumed in February. 
This time I’ve ordered them roughly on how good they are.&lt;/p&gt;

&lt;p&gt;** Films&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spotlight&lt;/li&gt;
  &lt;li&gt;The Big Short&lt;/li&gt;
  &lt;li&gt;What We Do In The Shadows&lt;/li&gt;
  &lt;li&gt;Edge of Tomorrow&lt;/li&gt;
  &lt;li&gt;Alien and Aliens&lt;/li&gt;
  &lt;li&gt;Bridge of Spies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spotlight and The Big Short are going to find themselves tough to beat in 2016. Both stellar films highlighting the state of corruption in our society albeit with different tones.&lt;/p&gt;

&lt;p&gt;Moving onto the more lighthearted films, What We Do In The Shadows (WWDITS) is an original, low budgety comedy that will make you laugh. It takes on the mockumentary approach adds a supernatural spin. It takes all the good parts from The Inbetweeners, The Office and vampire lore and package them into a film. This is what WWDITS essentially is. I look forward to a sequal and more from the director. 
Speaking again of originallity, Edge of Tomorrow will take any viewer back to there childhood. It’s got robots, aliens, high tech weaponary and a respawning mechanic. If you want a film to watch on a saturday night after the X-Factor has finished you’d be hard pressed to find something as enjoyable as Edge of Tomorrow.&lt;/p&gt;

&lt;p&gt;Bridge of Spies fell a bit to me. Like last months Legend viewing, Bridge of Spies couldn’t deciede whether it wanted to be a courtroom drama or more focused on the negotiation aspect between East Germany and Russia. As a result you got two unsatisfactory parts of the film. It also had the usual Spielberg quirks: a moment of two people talking over each other, the main character going from being hated to being loved as if public opinion is that black and white. By all means it is a good film, but nothing more. Watch it once and continue with your life.&lt;/p&gt;

&lt;p&gt;Finally, inspired by a play-through of Alien: Isolation on the PS4 I decieded to eduacte myself in the actual films. Alien had the very ‘classical’ sci-fi feel to it, with the beeping terminals, robots so realistic they pass of as humans and finally actual people in rubber suits opposed to CGI. Watching it takes you into that world and makes you nostalgic for the alternate timeline where computers fill rooms and sleeping pods. The influence it has on cinema is easily digested and thankfully doesn’t move into the ‘Seinfield is unfunny’ trope. Similarly, Aliens takes the same angle, although much less delicate. There are even paralels to Edge of Tomorrow and Aliens.&lt;/p&gt;

&lt;p&gt;** Books&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Boys&lt;/li&gt;
  &lt;li&gt;Chris Hadfield’s Autobiography&lt;/li&gt;
  &lt;li&gt;The Art of Deception&lt;/li&gt;
  &lt;li&gt;Crytopnomicon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout January I was a slave to the behomoth that is Cryprtonomicon and I’m ashamed to say that I still haven’t finished it. I’m 600 pages in and still feel like the proper story is yet to start. Everynow and then one of the characters goes on an arc that has me turning pages, but that usually finishes at the end of the chapter and then the book loses momentum and I find myself looking at the back trying to work out how many pages I need to read a day to finish before the next leap year. For this reason I think I’m going to have to relegate Crytonomicon to the unfinished pile.&lt;/p&gt;

&lt;p&gt;Moving onto the non-fiction books it has been a mixture of finance, space and security. 
With the debate of IEX continually appearing on my Twitter timeline and my Phd work touching it’s toes in microstructure, Flash Boys has been on my to-read list for a while. I’m still strugling to form an opinion on the morality of HFT even after reading it. I feel like the majority of what is described in Flash Boys can be categorised as market impact. If you place a large order, other traders are going to adjust there prices. The finite nature of passing messages back and forth from exchanges means that there will be information asymetry and thus arbitrage oppurunities.&lt;/p&gt;

&lt;p&gt;From low latency to high latency, Chris Hadfield’s book is a simple and consice account of his time training and actual time in space on various missions. I enjoyed it for the insight into the organisation of NASA and how as a Canadian there is a slightly different path to becoming a spaceman. I imagine it’s similar to the UK and becoming part of the ESA crew. 
Whilst the book is an easy read, I found it a bit light on the details and prehaps a bit of negativity to give it a bit of balance. It felt much like a good PR read for NASA and many of it’s weaknesses were actually it’s strenghts. I.e. the flat organisational structure that means that returning astronaughts are placed bottom of the pile. Much like Alex Fergusan said, no player bigger than the club - no astronaught bigger than NASA.&lt;/p&gt;

</description>
        <pubDate>Fri, 09 Jun 2017 17:55:25 +0100</pubDate>
        <link>/2017/06/09/FebFilms.html</link>
        <guid isPermaLink="true">/2017/06/09/FebFilms.html</guid>
        
        
      </item>
    
      <item>
        <title>Bayesian Autoregressive Processes</title>
        <description>&lt;p&gt;An autoregressive process can be described by the equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_t = c + \phi y_{t-1} + \epsilon.&lt;/script&gt;

&lt;p&gt;The parameter &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is some baseline, &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; if between -1 and 1, and &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is some white noise process. If we consult the Wikipedia artivle on such process we find that there it is fairly trivial to calculate the unknown parameter &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; in a frequentist setting. Googling about for a Bayesian introduction didn’t turn up anything particulary helpful, so here I try to plug that gap.&lt;/p&gt;

&lt;p&gt;For any Bayesian method we need to decompose our problem into three parts; likelihood, prior and posterior distribution. For simplicity we will be setting &lt;script type=&quot;math/tex&quot;&gt;c=0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For the likelihood we can see that each observation &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is normally distributed around &lt;script type=&quot;math/tex&quot;&gt;\phi y_{i-1}&lt;/script&gt; with variance equal to that of the white noise process &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_i | y_1, \ldots , y_n , \sigma _\epsilon ^2) \propto \frac{1}{\sigma _\epsilon} \exp \left( \frac{-(y_i - \phi y_{i-1})^2}{2 \sigma _\epsilon ^2} \right),&lt;/script&gt;

&lt;p&gt;now the likelihood is just this density multiplied across all the data.&lt;/p&gt;

&lt;p&gt;Now for the prior. Like any Gaussian inference problem it is a smart choice to use a Gaussian prior on &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; so that we get a conjugate prior. But there is a hard limit on the values of the parameter in question &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
-1 &lt; \phi  &lt; 1 %]]&gt;&lt;/script&gt;, therefore we must use the truncated normal distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\phi | \mu _0 , \sigma _0 ^2) = \frac{\exp \left( - \frac{ (\phi - \mu_0) ^2}{2 \sigma _0 ^2} \right)}{\sqrt{2 \pi} \sigma_0 \left(\Phi \left( \frac{b-\mu_0}{\sigma _0} \right) - \Phi \left( \frac{a-\mu_0}{\sigma _0} \right) \right) }&lt;/script&gt;

&lt;p&gt;the values of &lt;script type=&quot;math/tex&quot;&gt;a , b&lt;/script&gt; set the limits of the truncation, so in our case they will be &lt;script type=&quot;math/tex&quot;&gt;-1, 1&lt;/script&gt; respectivley.&lt;/p&gt;

&lt;p&gt;So lets combine both the likelihood and the prior to get our posterior distribution for &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. Due to the conjugacy of the prior, we know that the posterior is also going to be a truncated normal distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p( \phi | y_1 , \ldots , y_n, \mu _0  , \sigma _0 ^2, \sigma _{\epsilon} ^2 ) = \text{Truncated-Normal} ( \mu  \sigma ^{2 } , \sigma ^2  )&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu  = \frac{\sum _i y_i y_{i-1}}{\sigma _\epsilon ^2} + \frac{\mu _0}{ \sigma _0 ^2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma ^{2 } = \left( \frac{\sum_i y_i ^2}{\sigma _\epsilon ^2} + \frac{1}{\sigma _0^2} \right)^{-1}&lt;/script&gt;

&lt;p&gt;Now these are simple enough to implement in a few lines of R and with such a simple model I’ll leave that as an exercise to the reader.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1611/1611.08747.pdf&quot;&gt;https://arxiv.org/ftp/arxiv/papers/1611/1611.08747.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Truncated_normal_distribution&quot;&gt;https://en.wikipedia.org/wiki/Truncated_normal_distribution&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;&gt;https://en.wikipedia.org/wiki/Autoregressive_model&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Jun 2017 00:00:00 +0100</pubDate>
        <link>/2017/06/09/autoregressive.html</link>
        <guid isPermaLink="true">/2017/06/09/autoregressive.html</guid>
        
        
      </item>
    
      <item>
        <title>Mark Clattenburg</title>
        <description>&lt;p&gt;The news has just broken that Premier League referee &lt;a href=&quot;https://www.premierleague.com/news/334134&quot;&gt;Mark Clattenburg is moving to Saudi Arabia&lt;/a&gt;. Having refereed the FA Cup final, Champions League Final and Euro final last year alone he can quite comfortably say he has done it all. As they say, you are only as good as the last game you refereed, so perhaps moving somewhere less visible is a good end to an exemplary career.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/markclattenburg.png&quot; alt=&quot;Mark Clattenburg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Across both seasons (or season and a half) he has been consistent in the number of red cards for both home and away. The same with his foul counts. Quite a large difference in the amount of yellow cards though, the only thing that is different between the two seasons.&lt;/p&gt;

&lt;p&gt;A good comparison can be made between Clattenburg and Howard Webb in their ‘peak’ years. Howard Webb refereed the World Cup Final and Champions League final in the 2009/10 season (missing out on the FA Cup final to Chris Foy).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/mchwradar.png&quot; alt=&quot;Clattunberg and Webb Radar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Webb was average for fouls throughout that season but more active with the yellow cards and under active with the red cards. Overall it is quite the rounded radar, nothing really striking out. Whereas Clattenburg’s with the high amount of away yellow cards is a bit more jagged. The difference in shapes between the two referees gives an indication of the different styles and personalities of the referees.&lt;/p&gt;

&lt;p&gt;Overall, the Premier League will be missing out on a good referee, but it is probably a sensible decision for Mark to take, he’s reached the top, time to take some of the rewards.&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
        <link>/2017/02/16/Mark-Clattenburg.html</link>
        <guid isPermaLink="true">/2017/02/16/Mark-Clattenburg.html</guid>
        
        
      </item>
    
      <item>
        <title>An Introduction to Julia and Distributions</title>
        <description>&lt;p&gt;Julia is a new language on the block aimed at being a suitable mid point between the adaptability of Python and the speed of Matlab. Its a nice fall-back when my R code is just that bit too slow to really churn through some numbers.&lt;/p&gt;

&lt;p&gt;On of the main benefits of using R is the ease at which the ‘standard’ distributions are available. Want exponentially distributed random variables? Just call rexp()! Want the pdf of the gamma distribution? dgamma() is there to help you. With Julia this type of functionality is in the &lt;a href=&quot;https://github.com/JuliaStats/Distributions.jl&quot;&gt;Distribution module&lt;/a&gt;, so takes just a little bit more of work to get the same functionality.&lt;/p&gt;

&lt;p&gt;In this post I will outline how the basics of the distributions package and how you can replicate some of the functionality of R.&lt;/p&gt;

&lt;p&gt;Firstly, we need to install the Distributions package. This is done by calling &lt;code class=&quot;highlighter-rouge&quot;&gt;Pkg.add(&quot;Distributions&quot;)&lt;/code&gt;. Now that is installed we need to load it into the namespace. Open a new Julia instance and load the package with &lt;code class=&quot;highlighter-rouge&quot;&gt;using Distributions&lt;/code&gt;. The necessary functions are now loaded.&lt;/p&gt;

&lt;p&gt;Our first exercise will be to sample &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; exponentially distributed variables and check that the density of the samples tends to the pdf of the exponential distribution as &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; becomes larger. 
The first step in this code is to define our distribution. As the exponential distribution only requires one parameter, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, this is as simple as calling &lt;code class=&quot;highlighter-rouge&quot;&gt;Exponential(m)&lt;/code&gt; in our code. Now we use a number of different functions on the distribution.&lt;/p&gt;

&lt;p&gt;We can sample from this distribution using &lt;code class=&quot;highlighter-rouge&quot;&gt;rand(dist, N)&lt;/code&gt; where &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; is the number of samples to draw. We can then overlay the pdf of the distribution by using &lt;code class=&quot;highlighter-rouge&quot;&gt;pdf(dist, x)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Combing these commands allows us to draw a graph (using the Julia package &lt;a href=&quot;http://gadflyjl.org/stable/&quot;&gt;Gadfly&lt;/a&gt;) like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/expPlot.svg&quot; alt=&quot;Exponential Plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we can see the small sample size does not resemble the pdf but the large sample size does. So we are correctly drawing from the exponential distribution as expected.&lt;/p&gt;

&lt;p&gt;There are also other functions available. A great example is calculating the mean of a log-normal distribution. This distribution is defined with two parameters; &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s^2&lt;/script&gt;. However, the mean of the distribution is not equal to &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;. Instead it is &lt;script type=&quot;math/tex&quot;&gt;\exp(m+\frac{s^2}{2})&lt;/script&gt;. The Distributions package in Julia knows this. So by simply calling &lt;code class=&quot;highlighter-rouge&quot;&gt;mean&lt;/code&gt; on the &lt;code class=&quot;highlighter-rouge&quot;&gt;LogNormal()&lt;/code&gt; object you can return the theoretical mean of the distribution and not have to worry about the parametrisation specifics of the distribution.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dist = LogNormal(1,4)
mean(dist) == exp(1 + 4^2/2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Overall, Julia and the Distributions package offer similar functionality to R. You can easily replicate some of the functions in R with very effort in Julia. This can be a useful tool if R is not quite cutting it on the speed front.&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
        <link>/2017/01/26/Julia-Distributions.html</link>
        <guid isPermaLink="true">/2017/01/26/Julia-Distributions.html</guid>
        
        
      </item>
    
      <item>
        <title>Mike Dean and the Ref Radar</title>
        <description>&lt;p&gt;It was a tough day at the office last week for Mike Dean. The match was Manchester United vs West Ham and after 15 minutes he dismissed Feghouli for a foul tackle. The tempo of the game changed. West Ham were on the back foot and eventually conceded two goals to Man Utd. Undeniably the red card had a big effect on the outcome of the game. It has since been rescinded, showing that the FA agree with fans that perhaps it should not have been a red card.&lt;/p&gt;

&lt;p&gt;There has also been a tweet circulating that Mike Dean in his last 7 Tottenham games has sent off three opposition players. On first reading this raises some eyebrows. Is this just a statistical oddity or is there something deeper in the numbers? A prime opportunity to explore our RefRadar of Mike Dean and see how he compares to the other Premier League referees.&lt;/p&gt;

&lt;p&gt;Lets look at the season so far (2016/17) in isolation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadar.png&quot; alt=&quot;Mike Dean Ref Radar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So as we can see here, Mike Dean does like penalising the away team more than the home team for both fouls and yellow cards. Obviously these two stats are going to be correlated. But it is still interesting to see how he is fairly close to the average amount for penalising the home team, but for the away team he is frequently giving fouls and yellow cards out.&lt;/p&gt;

&lt;p&gt;Mike Dean has officiated the 5th most amount of games in the league. So let’s look at the 3 other referees who have had more games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/MikeDeanRefRadarComp.png&quot; alt=&quot;Mike Dean Ref Radar Comp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most notably, three out of four of the most active referees this season are the ones giving the most yellow cards to the away teams whereas they are middle of the pack for the home yellow cards. A similar trend is seen for the away fouls, but like previous, these two variables are going to be highly correlated.
The stand out statistic though is Mike Deans average away red cards. He is a clearly the most likely to send off a player of the away team out of this selection of referees. In fact the three other referees haven’t sent anyone off from the away team yet this season! But something to keep in mind is that this is a really small sample size. The season is about half way through so there is still plenty of time for the other referees to catch up on the red cards.&lt;/p&gt;

&lt;p&gt;So in conclusion, whatever match Mike Dean is appointed to next, the away team should keep in my mind a red card is never far away. But perhaps the FA are going to have a word with Mike Dean and he will have an easy couple of games to bring his average down. The RefRadar here has been useful to look at how active the officials have been compared to their peers. Lets see what the next round of football brings!&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
        <link>/2017/01/07/Mike-Dean-Ref-Radar.html</link>
        <guid isPermaLink="true">/2017/01/07/Mike-Dean-Ref-Radar.html</guid>
        
        
      </item>
    
      <item>
        <title>Introducing the Referee Radar</title>
        <description>&lt;p&gt;The radar plot is a good way to analyse different metrics across a group. The football player radar made popular by &lt;a href=&quot;http://statsbomb.com/2016/04/understand-football-radars-for-mugs-and-muggles/&quot;&gt;Ted Knutson&lt;/a&gt; is good at comparing players and seeing how their stats stack up against one another. Here I will be taking a similar concept and using the radar plot to analyse the referee’s in the professional game in England.&lt;/p&gt;

&lt;p&gt;First, we need to chose some metrics. Using the data from &lt;a href=&quot;http://www.football-data.co.uk/&quot;&gt;Football-Data&lt;/a&gt; we can download all the league matches from the last three seasons. In this data we are privy to the number of fouls, yellow cards and red cards for both the home and away team. That gives us 6 variables for each match and each referee, the perfect amount for a radar plot.&lt;/p&gt;

&lt;p&gt;For each referee we can calculate the average amount of fouls, yellow and red cards the gave to both the home and away team. This will allow us to detect whether a referee is particularly card happy or even has a home or away bias. In terms of practicality, we have to set a threshold for minimum number of games officiated. We remove any ref that has refereed less than 20 games over the three seasons.&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://github.com/ricardo-bion/ggradar&quot;&gt;ggradar&lt;/a&gt; package  and tweaking some of the graphical parameters we are able to come up with the following plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/RefereeRadar.png&quot; alt=&quot;RefereeRadar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we are comparing four referees (at random) and how their metrics personally match against the population of all referees. Here we can see Andy D’Urso is particular for sending an away player off. Simon Hooper is quite a stickler for a foul. Nigel Miller is laid back, not giving many fouls and not giving out the yellow cards either. Chris Kavanagh is very middle of the pack, consistent across home and away for both fouls and cautions.&lt;/p&gt;

&lt;p&gt;The package &lt;code class=&quot;highlighter-rouge&quot;&gt;ggradar&lt;/code&gt; requires all the variables to have a consistent range. For this I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;rescale&lt;/code&gt; function of R to remap the averages to &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;. Therefore the actual values of the metrics are lost in the radar plot. This is something I’ll work on to include in future versions. I’ll also be creating an app, either in shiny or JavaScript that will allow users to compare different referees as they see fit.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/24/Ref-Radar-Intro.html</link>
        <guid isPermaLink="true">/2016/12/24/Ref-Radar-Intro.html</guid>
        
        
      </item>
    
      <item>
        <title>The Data Dialogue - At War with Data</title>
        <description>&lt;p&gt;On Wednesday I took a stroll down to King’s College across the river for a seminar on data and warfare. A bit of a broad subject for a conference. The seminar speakers came from a variety of fields where the innovative usage of data is less obvious and more unique.&lt;/p&gt;

&lt;p&gt;The first talk was by &lt;a href=&quot;http://www.kcl.ac.uk/hr/diversity/meettheprofessors/artshums/croueche.aspx&quot;&gt;Charlotte Rouche&lt;/a&gt; of the Kings college archaeological department. She highlighted how maps, both ancient and current, play an important role in her archaeological research. Understanding how the land is divided and even the names of places in various languages gives an important insight into how civilisation has evolved throughout the ages. Mapping data is now ubiquitous with the likes of Google Maps and Google Earth and this can be useful in observing how areas have developed over time. The aerial photographs of sites of archaeological interest can be periodically monitored for change and help with preservation - fighting the war against decay. However, when cataloguing such data Prof. Rouche raised the importance question of accessibility . Should the public be allowed to see aerial photographs and accurate co-ordinates of culturally and theologically significant monuments? In times of war, this could potentially be a shopping list for attacks. This shows how perhaps a fully open data source is not always the correct answer.&lt;/p&gt;

&lt;p&gt;The second talk was by &lt;a href=&quot;https://www.ucl.ac.uk/spacetimelab/people/kate-bowers&quot;&gt;Kate Bowers&lt;/a&gt; of the Crime Science department at UCL. Her talk was on the usage of police data, which has similar disclosure properties to the Rouche’s map data. Prof. Bowers showed how the GPS data collected from police cars can provide maps of frequent patrol paths of the Metropolitan police. Combining this with 999 call monitoring she was able to give an indication of where there was deficiency in policing were arising. This can lead to policy implementation and a more efficient patrol path for police officers. In this case the war on crime was being fought with this new data.&lt;/p&gt;

&lt;p&gt;The third talk was by &lt;a href=&quot;https://kclpure.kcl.ac.uk/portal/robert.stewart.html&quot;&gt;Robert Stewart&lt;/a&gt;. He is a mental health doctor from the NHS and Professor at King’s. Interestingly, his trust has developed a searchable, anonymous database for mental health researchers. This allowed them to query various parameters across patient groups. A very interesting research tool that required individual patient consent but once granted provides researchers with large amounts of real world clinical data. Its success shows that it is possible to use the health data of patients anonymously without infringing on privacy issues. Currently it is being used to help study drug safety and other questions that required long term monitoring.  Now I have previously worked for a NHS trust in the IT department and have an understanding of different software systems available for the NHS and the various data sharing regulations in place. Therefore this talk was a good insight into how other trusts can conquer some of the data sharing rules and how researchers can get access to such data. As Prof. Stewart is a mental health doctor his work can be framed as the war against mental afflictions.&lt;/p&gt;

&lt;p&gt;The final talk was based on finding adversaries on the internet and looking at how such actors can make themselves known unwillingly. This talk was given by &lt;a href=&quot;http://www.kcl.ac.uk/sspp/departments/warstudies/people/professors/rid.aspx&quot;&gt;Thomas Rid&lt;/a&gt; from the War Studies department at King’s. He offered the example of the Poseta email &lt;a href=&quot;https://en.wikipedia.org/wiki/Podesta_emails&quot;&gt;leak&lt;/a&gt;. By using the classic “reset you password here” phishing scam, Podesta clicked on a bitly link that populated a fake web page with his gmail information and required him to submit his password to change it. Instead, the changing the password button just forwarded his password to the adversary. We know that this was the vector chosen by the attackers due to the public nature of the bitly account. Whoever set the account up forgot to set the profile to private. This led to the account being traced back  to some actor. Who it actually was is up to speculation at the time of writing.&lt;/p&gt;

&lt;p&gt;Overall, it was an enjoyable conference. I learnt about some new domains and in a more focused and “apply-side” way. It was less about algorithm run times and more about what the algorithms actually do and how they can be used.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/07/DataDialogue.html</link>
        <guid isPermaLink="true">/2016/12/07/DataDialogue.html</guid>
        
        
      </item>
    
      <item>
        <title>Notes from a Quantcast Talk</title>
        <description>&lt;p&gt;As I sit on the train to London waiting for my model to finish fitting I realised that it had been a while since I had written a blog post.&lt;/p&gt;

&lt;p&gt;My PhD school holds a bimonthly seminar and this weeks guest speaker was Dr. Peter Day who is a director of engineering at &lt;a href=&quot;https://www.quantcast.com&quot;&gt;Quantcast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now Quantcast is a 21st century advertising agency offering real time advertisement strategy and analytic tools. The lecture was based around their use of data and how statistics and infrastructure has helped make advertising more relevant and serve better adds.&lt;/p&gt;

&lt;p&gt;My first &lt;code class=&quot;highlighter-rouge&quot;&gt;did you know&lt;/code&gt; was that the adverts you see on websites are sold in real time as you load the page. So in the time it takes from clicking a link on Goole to the page appearing in your browser, the add space has been bought microseconds earlier and now serving a specific add designed for you. A remarkable engineering achievement, that in the short time it takes from clicking a link to seeing a page that an auction takes place, a winner is found and the add is served. But this is not the main product of Quantcast.&lt;/p&gt;

&lt;p&gt;Instead, Quantcast is more about finding out more about population behaviours and how effective that advert will be. They crunch the necessary data from the variety of cookies they collect and come up with a pricing strategy based on who might see the add. Where as one company might try to show the add to as many people as possible (the fire-hose strategy) Quantcast might drill down on certain factors and change their bidding price based on these factors.&lt;/p&gt;

&lt;p&gt;This problem is the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;big data&lt;/code&gt; problem. The amount of data you can collect from cookies; visited websites, location, device etc. can lead to many other inferences which gives a large amount of variables for a large amount of people. Computing this information lead to Quantcast developing their own database and data tools as the current market leaders (Hadoop and derivatives) where not performing well enough. Although they did hint that they was looking at moving to AWS to solve some of their infrastructure problems.&lt;/p&gt;

&lt;p&gt;Overall, it was an interesting talk about a field that I hadn’t really paid that much attention too previously and I definitely learnt something new. It also made me thankful of my add blocker and slightly more paranoid. So thank to Quantcast for coming down and delivering an enjoyable seminar!&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
        <link>/2016/12/02/Quantcast.html</link>
        <guid isPermaLink="true">/2016/12/02/Quantcast.html</guid>
        
        
      </item>
    
  </channel>
</rss>
